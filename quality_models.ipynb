{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"opp4Cld9gck7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653062590046,"user_tz":-60,"elapsed":7475,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}},"outputId":"f223e8e1-8a11-4e21-ba46-1d89729b1916"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda:0\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fa730e280b0>"]},"metadata":{},"execution_count":6}],"source":["!pip install -r /content/drive/MyDrive/Dissertation/requirements.txt -qqq\n","!pip install -Uqq ipdb\n","\n","\n","from datetime import datetime\n","import json\n","import torch\n","import torchvision\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import os\n","import numpy as np\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import argmax\n","from tqdm import tqdm\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import seaborn as sn\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score, classification_report, balanced_accuracy_score\n",")\n","from torchsampler import ImbalancedDatasetSampler\n","import wandb\n","import numpy as np\n","from os import listdir\n","from os.path import join, isdir\n","from glob import glob\n","import cv2\n","import timeit\n","import timm\n","from sklearn.metrics import confusion_matrix\n","from torchvision.datasets import ImageFolder\n","from collections import Counter\n","from sklearn.utils import class_weight\n","# from autoaugment import ImageNetPolicy\n","# from autoaugment import CIFAR10Policy\n","# from autoaugment import SVHNPolicy\n","from torchvision.transforms import AutoAugment\n","\n","from torchvision.transforms.autoaugment import AutoAugmentPolicy\n","\n","#changing the device to GPU rather than CPU if it is available,\n","# this will decrease model training time.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device {}\".format(device))\n","\n","torch.manual_seed(17)"]},{"cell_type":"markdown","metadata":{"id":"-qLIgh8FkgiD"},"source":["Several custom classes were made. The first, ImageResize(), resizes an image using the PIL image resizer set to the bilinear function. A custom class was generated even though Pytorch has its own resizer, as discussed in the following article, https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing there are resizing issues associated with certain libraries such as Pytorch which introduce artefacts or don't provide sufficient antialiasing in the resized images using libraries such as Pytorch.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"b9TywHjhgu2P","executionInfo":{"status":"ok","timestamp":1653062590049,"user_tz":-60,"elapsed":82,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["class PILtoCV2(object):\n","\n","    def __call__(self, image):\n","        return cv2.cvtColor(\n","            np.array(image),\n","            cv2.COLOR_RGB2BGR\n","        )\n","\n","\n","class CV2toPIL(object):\n","\n","    def __call__(self, image):\n","        color_coverted = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return Image.fromarray(color_coverted)\n","\n","\n","class ImageResize(object):\n","        \"\"\"\n","        PIL's resize performs better than pytorch\n","        https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing\n","        \"\"\"\n","\n","        def __init__(self, new_h, new_w):\n","            self.new_h = new_h\n","            self.new_w = new_w\n","\n","        def __call__(self, image):\n","            image = image.resize((self.new_w, self.new_h), resample=Image.BILINEAR)\n","            return image\n","\n","class HairRemoval(object):\n","        \"\"\"\n","        Hair removal code\n","        https://github.com/ThiruRJST/Melanoma_Classification \n","        \"\"\"\n","\n","        def __call__(self, image):\n","            start_time = datetime.now()\n","            # convert image to grayScale\n","            grayScale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","            # kernel for morphologyEx\n","            kernel = cv2.getStructuringElement(1, (17, 17))\n","\n","            # apply MORPH_BLACKHAT to grayScale image\n","            blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","\n","            # apply thresholding to blackhat\n","            _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n","\n","            # inpaint with original image and threshold image\n","            final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n","            # print(\"Hair removal\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","            return final_image\n","\n","\n","class ShadesOfGrey(object):\n","        \"\"\"\n","        Code from https://github.com/ThiruRJST/Melanoma_Classification\n","        imgage (numpy array): the original image with format of (h, w, c)\n","        power (int): the degree of norm, 6 is used in reference paper\n","        gamma (float): the value of gamma correction, 2.2 is used in reference paper\n","        \"\"\"\n","\n","        def __init__(self, power=6, gamma=2.2):\n","            self.power = power\n","            self.gamma = gamma\n","\n","        def __call__(self, image):\n","            start_time = datetime.now()\n","            image_dtype = image.dtype\n","\n","            if self.gamma is not None:\n","                image = image.astype('uint8')\n","                look_up_table = np.ones((256, 1), dtype='uint8') * 0\n","                for i in range(256):\n","                    look_up_table[i][0] = 255 * pow(i / 255, 1 / self.gamma)\n","                image = cv2.LUT(image, look_up_table)\n","\n","            image = image.astype('float32')\n","            image_power = np.power(image, self.power)\n","            rgb_vec = np.power(np.mean(image_power, (0, 1)), 1 / self.power)\n","            rgb_norm = np.sqrt(np.sum(np.power(rgb_vec, 2.0)))\n","            rgb_vec = rgb_vec / rgb_norm\n","            rgb_vec = 1 / (rgb_vec * np.sqrt(3))\n","            image = np.multiply(image, rgb_vec)\n","\n","            # Andrew Anikin suggestion\n","            image = np.clip(image, a_min=0, a_max=255)\n","            # print(\"Shades of grey\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","            return image.astype(image_dtype)\n","    \n","class CropBlackCircle(object):\n","    \"\"\"\n","    https://stackoverflow.com/questions/61986407/crop-x-ray-image-to-remove-black-background \n","    \"\"\"\n","    def __call__(self, image):\n","        start_time = datetime.now()\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","        # threshold \n","        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n","        hh, ww = thresh.shape\n","\n","        # make bottom 2 rows black where they are white the full width of the image\n","        thresh[hh-3:hh, 0:ww] = 0\n","\n","        # get bounds of white pixels\n","        white = np.where(thresh==255)\n","        xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n","\n","        # crop the image at the bounds adding back the two blackened rows at the bottom\n","        crop = image[ymin:ymax+3, xmin:xmax]\n","        # print(\"Crop black circle\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","        return crop\n","\n","class CutOut(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","        Args:\n","            n_holes (int): Number of patches to cut out of each image.\n","            length (int): The length (in pixels) of each square patch.\n","        \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        #start_time = datetime.now()\n","        img = np.array(img)\n","        # print(img.shape)\n","        h = img.shape[0]\n","        w = img.shape[1]\n","\n","        mask = np.ones((h, w), np.uint8)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        #mask = torch.from_numpy(mask)\n","        #mask = mask.expand_as(img)\n","        img = img * np.expand_dims(mask,axis=2)\n","        #print(\"Cut out\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","        return img  "]},{"cell_type":"markdown","metadata":{"id":"ZeVHonwolOyZ"},"source":["## Network generation\n","\n","Five networks were created using several architectures loaded from the Pytroch library.  Within each model-specific function, the feature extraction layers have been frozen and the final classification layer unfrozen, to allow this final layer to train on our Lesion dataset."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bxMWZ9CIlK7B","executionInfo":{"status":"ok","timestamp":1653062590051,"user_tz":-60,"elapsed":81,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["def create_network_densenet():\n","        net = models.densenet161(pretrained=True)\n","        net.classifier = nn.Linear(in_features=2208, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n","\n","\n","def create_network_efficientnet():\n","        net = models.efficientnet_b0(pretrained=True)\n","        net.classifier[1] = nn.Linear(in_features=1280, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n","\n","def create_network_mobilenet():\n","        net = models.mobilenet_v3_small(pretrained=True)\n","        net.classifier[3] = nn.Linear(in_features=1024, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","            param.requires_grad = False\n","        for param in net.classifier.parameters():\n","            param.requires_grad = True\n","\n","        return net\n","\n","\n","def create_network_resnet():\n","        net = models.resnet18(pretrained=True)\n","        net.fc = nn.Linear(in_features=512, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","            param.requires_grad = False\n","        for param in net.fc.parameters():\n","            param.requires_grad = True\n","\n","        return net\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xDTi6y0Uk7N9","executionInfo":{"status":"ok","timestamp":1653062590053,"user_tz":-60,"elapsed":81,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["def run(config):  \n","    torch.manual_seed(17)\n","    BASE_PATH = '/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_1_models/'\n","    now = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\")\n","    RESULTS_PATH = os.path.join(BASE_PATH, now)\n","    os.makedirs(RESULTS_PATH, exist_ok=False)\n","\n","    with open(os.path.join(RESULTS_PATH, \"config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=4)\n","\n","    if config.get(\"use_wandb\"):\n","        # this integrates the third-party platform, Weights and Biases,\n","        # with this notebook.\n","        run = wandb.init(\n","            project=\"part_1_skin_lesion\",\n","            entity=\"sarahlouise\",\n","            config=config,\n","        )\n","\n","\n","    # ## Train, validation and test set\n","    #\n","    # The train, validation and test sets were loaded and transformed using the Pytorch functions and amended classes previously discussed.\n","\n","    batch_size = config[\"batch_size\"]\n","    Imagenet_NV = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","    print(\"Configuring datasets\")\n","    trainset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'train'),\n","        transform=transforms.Compose([\n","                            transforms.RandomVerticalFlip(0.5),\n","                            transforms.RandomHorizontalFlip(0.5),\n","                            transforms.RandomApply(\n","                                nn.ModuleList([\n","                                               transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n","                                               transforms.RandomRotation(degrees=(0, 360)),\n","                                               transforms.CenterCrop(100)\n","                                               ]), p=0.1),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","\n","                        ]))\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        trainset,\n","        batch_size=batch_size,\n","        num_workers=2,\n","        shuffle = False if config.get(\"use_imbalanced_sampler\") else True,\n","        sampler=ImbalancedDatasetSampler(trainset, callback_get_label=lambda x: x.targets) if config.get(\"use_imbalanced_sampler\") else None, \n","    )\n","\n","    valset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'val'),\n","        transform=transforms.Compose([\n","                            # PILtoCV2(),\n","                            # HairRemoval(),\n","                            # CropBlackCircle(),\n","                            # ShadesOfGrey(),\n","                            # CV2toPIL(),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","                        ]))\n","\n","    valloader = torch.utils.data.DataLoader(\n","        valset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","\n","    testset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'test'),\n","        transform=transforms.Compose([\n","                                # PILtoCV2(),\n","                                # HairRemoval(),\n","                                # CropBlackCircle(),\n","                                # ShadesOfGrey(),\n","                                # autoaugment(AutoAugmentPolicy.IMAGENET),\n","                                # CV2toPIL(),\n","                                ImageResize(224,224),\n","                                transforms.PILToTensor(),\n","                                transforms.ConvertImageDtype(torch.float),\n","                                transforms.Normalize(*Imagenet_NV),\n","                            ]))\n","\n","    testloader = torch.utils.data.DataLoader(\n","        testset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","    \n","    classes = sorted(os.listdir(os.path.join(config.get(\"data_path\"), 'train')))\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.sklearn.plot_class_proportions(\n","            trainset.targets,\n","            testset.targets,\n","            classes\n","        )\n","\n","\n","    #making logical arguments from the configuration dictionary definied earlier\n","    model_name = config[\"model\"]\n","\n","    if model_name == \"densenet\":\n","        net = create_network_densenet()\n","    elif model_name == \"efficientnet\":\n","        net = create_network_efficientnet()\n","    elif model_name == \"mobilenet\":\n","        net = create_network_mobilenet()\n","    elif model_name == \"resnet\":\n","        net = create_network_resnet()\n","    else:\n","        raise ValueError(\"Model name not supported '{}'\".format(model_name))\n","\n","    print(f\"Moving network to {device}\")\n","    net = net.to(device)\n","\n","    # ## Training configurations\n","    \n","    # defining the training configurations\n","    no_of_epochs = config[\"epochs\"]\n","    \n","    # calculating class weights\n","    labels = np.array(trainset.targets)\n","    class_weights = class_weight.compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(labels),\n","        y=labels\n","    )\n","    class_weights = torch.tensor(\n","        class_weights,\n","        dtype=torch.float\n","    )\n","\n","    if config.get(\"use_weighted_loss\"):\n","        criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n","    else:\n","        criterion = nn.CrossEntropyLoss().to(device)\n","\n","    optimizer = optim.Adam(\n","        net.parameters(),\n","        lr=config[\"lr\"],\n","        amsgrad=config[\"use_amsgrad\"]\n","    )\n","\n","    if config[\"use_warm_restarts\"]:\n","        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","            optimizer,\n","            T_0=10,\n","            T_mult=1,\n","        )\n","    else:\n","        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","            optimizer,\n","            round((len(trainset)/batch_size)*no_of_epochs)\n","        )\n","\n","\n","    # ##  Model training\n","    #\n","    # This section included the training for the model. The network was set to train, a mini-batch was passed through the model and then the loss was then backpropagated. Each trained model was also automatically saved.\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.watch(net)\n","\n","    if not config.get(\"use_pretrained\"):\n","        print(\"Starting training\")\n","        for epoch in range(no_of_epochs):  # loop over the dataset multiple times\n","            print(\"Epoch {} of {}\".format(epoch+1, no_of_epochs))\n","            epoch_running_loss = []\n","            epoch_val_metric = []\n","            for i, data in tqdm(enumerate(trainloader, 1), total=len(trainloader)):\n","                current_step = (len(trainloader) * epoch) + i\n","                \n","                # get the inputs; data is a list of [inputs, labels]\n","                net.train()\n","                \n","                inputs, labels = data\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                \n","                # forward + backward\n","                outputs = net(inputs)\n","\n","                \n","                # outputs = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","                \n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                # saving loss statistics\n","                loss_val = loss.item()\n","                if config.get(\"use_wandb\"):\n","                    wandb.log(\n","                        {\n","                            \"train/bce_logits_loss\": loss_val,\n","                            \"train/lr\": scheduler.get_last_lr()[0],\n","                            \"train/custom_step\": (len(trainloader) * epoch) + i,\n","                        }\n","                    )\n","\n","                # adding in valiation data testing\n","                if i % 40 == 0:\n","                    model_name = \"{name}-epoch-{epoch}-step-{step}.pth\".format(\n","                        name=config[\"name\"],\n","                        epoch=epoch,\n","                        step=i,\n","                    )\n","                    model_path = os.path.join(RESULTS_PATH, model_name)\n","                    torch.save(net.state_dict(), model_path)\n","\n","                    predicted = []\n","                    actual = []\n","\n","                    print(\"Running validation\")\n","                    net.eval()\n","                    with torch.no_grad():\n","                        for i, data in enumerate(tqdm(valloader, total=len(valloader))):\n","                            inputs, labels = data\n","                            inputs = inputs.to(device)\n","                            labels = labels.to(device)\n","\n","                            outputs = net(inputs)\n","                            loss = criterion(outputs, labels)\n","                            if config.get(\"use_wandb\"):\n","                                wandb.log({\n","                                    \"val/loss\": loss,\n","                                    \"val/custom_step\": current_step,\n","                                })\n","\n","                            # collect the correct predictions for each class\n","                            actual.extend(labels.detach().cpu().numpy())\n","                            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","\n","                        mb_acc = accuracy_score(actual, predicted)\n","                        precision = precision_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        recall = recall_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        bal_acc = balanced_accuracy_score(actual, predicted)\n","                        metric_report = classification_report(\n","                            np.array(actual),\n","                            np.array(predicted),\n","                            output_dict=True,\n","                            zero_division=1,\n","                        )\n","                        metric_report = {\n","                            f\"val_classes/{k}\": v\n","                            for k, v in metric_report.items()\n","                        }\n","                        metric_report.update({\n","                            \"val_classes/custom_step\": current_step,\n","                        })\n","\n","                        if config.get(\"use_wandb\"):\n","                            wandb.log({\n","                                \"val/accuracy\": mb_acc,\n","                                \"val/precision\": precision,\n","                                \"val/recall\": recall,\n","                                \"val/f1\": f1,\n","                                \"val/balanced_acc\": bal_acc,\n","                                \"val/custom_step\": current_step\n","                            })\n","                            wandb.log(metric_report)\n","\n","\n","\n","        print('Finished Training')\n","        # saving each trained model\n","        model_name = '{}-final.pth'.format(\n","            config[\"name\"]\n","        )\n","        model_path = os.path.join(RESULTS_PATH, model_name)\n","        torch.save(net.state_dict(), model_path)\n","\n","        print(\"Final model saved\")\n","\n","    # ##  Model testing\n","    #\n","    # The model was set to evaluate mode for testing. The predicted and actual results were saved, and used for generating a confusion matrix and metrics.\n","    #\n","    # A confusion matrix was generated, showing the number of true positives and negatives, and false positives and negatives. Additionally, the metrics were calculated, including the accuracy score, precision and recall. The corresponding graphs were created using Weights and Biases.\n","\n","\n","    if config.get(\"use_pretrained\"):\n","        net.load_state_dict(\n","            torch.load(\n","                config.get(\"use_pretrained\")\n","            )\n","        )\n","\n","    predicted = []\n","    actual = []\n","\n","    print(\"Testing model\")\n","    # again no gradients needed\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(tqdm(testloader, total=len(testloader))):\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = net(inputs)\n","            actual.extend(labels.detach().cpu().numpy())\n","            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","\n","    print(\"Calculating performance metrics\")\n","    model_accuracy = accuracy_score(actual, predicted)\n","    model_precision = precision = precision_score(actual, predicted,\n","                                                average=\"macro\", zero_division = 1)\n","    model_recall = recall_score(actual, predicted,\n","                                average=\"macro\", zero_division = 1)\n","    model_f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","    model_bal_acc = balanced_accuracy_score(actual, predicted)\n","    metric_report = classification_report(\n","        np.array(actual),\n","        np.array(predicted),\n","        output_dict=True,\n","        zero_division=1,\n","    )\n","    metric_report = {\n","        f\"test_classes/{k}\": v\n","        for k, v in metric_report.items()\n","    }\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.log({\"test/accuracy\": model_accuracy})\n","        wandb.log({\"test/precision\": model_precision})\n","        wandb.log({\"test/recall\": model_recall})\n","        wandb.log({\"test/f1\": model_f1})\n","        wandb.log({\"test/bal_acc\": model_bal_acc})\n","        wandb.sklearn.plot_confusion_matrix(actual, predicted, classes, normalize=\"true\")\n","        wandb.log(metric_report)\n","    else:\n","        print(\"test/accuracy\", model_accuracy)\n","        print(\"test/precision\", model_precision)\n","        print(\"test/recall\", model_recall)\n","        print(\"test/f1\", model_f1)\n","        print(\"test/bal_acc\", model_bal_acc)\n","\n","    plot_save_path = os.path.join(RESULTS_PATH, '{}_confusion_matrix.png'.format(\n","        config[\"name\"]\n","    ))\n","    print(\"Plotting confusion matrix in {}\".format(plot_save_path))\n","\n","    print(\"Calculating\")\n","    conf_matrix = confusion_matrix(y_true=actual, y_pred=predicted, normalize=\"true\")\n","    print(\"Done\")\n","    fig =plt.figure(figsize=(20, 20))\n","    ax = fig.add_subplot(1,1,1)\n","    ax.matshow(conf_matrix, cmap=plt.cm.Reds)\n","\n","\n","    plt.xlabel('Predictions')\n","    plt.ylabel('Actuals')\n","    plt.title('Confusion Matrix')\n","    plt.savefig(plot_save_path)\n","    # plt.show()\n","\n","    if config.get(\"use_wandb\"):\n","        run.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjSeEY5BUdey","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9f6c1789a27243dd84c02e5207ea710d","0262e36ab2d448518cf66f27d397273b","7e0a5dd8e7d7412fa845547eb09e014b","fa18ddb6aef748f7898ebd3891cd0528","1193178430e548379f9b3b8e049646cc","8fb04864afcc43a18b2487461e3eca31","b14a062cd3174b65824631427f5677e2","a9f677114fe3434c937d5526aa65a82f"]},"outputId":"69b65b13-d83c-4ceb-acf5-49e132a42661"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:22mht54y) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6c1789a27243dd84c02e5207ea710d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">twilight-hill-68</strong>: <a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion/runs/22mht54y\" target=\"_blank\">https://wandb.ai/sarahlouise/part_1_skin_lesion/runs/22mht54y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20220520_160206-22mht54y/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:22mht54y). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.16"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220520_160310-3ek9bj7n</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion/runs/3ek9bj7n\" target=\"_blank\">happy-surf-69</a></strong> to <a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Configuring datasets\n","Moving network to cuda:0\n","Starting training\n","Epoch 1 of 5\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▉     | 39/80 [02:58<03:32,  5.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:16<02:24, 16.10s/it]\u001b[A\n"," 20%|██        | 2/10 [00:17<00:57,  7.23s/it]\u001b[A\n"," 30%|███       | 3/10 [00:30<01:11, 10.21s/it]\u001b[A\n"," 40%|████      | 4/10 [00:31<00:38,  6.45s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:39<00:35,  7.09s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:40<00:19,  4.89s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:44<00:13,  4.46s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:44<00:06,  3.26s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:51<00:04,  4.51s/it]\u001b[A\n","100%|██████████| 10/10 [00:52<00:00,  5.25s/it]\n"," 99%|█████████▉| 79/80 [06:27<00:04,  4.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:08<01:16,  8.51s/it]\u001b[A\n"," 20%|██        | 2/10 [00:09<00:30,  3.85s/it]\u001b[A\n"," 30%|███       | 3/10 [00:16<00:38,  5.48s/it]\u001b[A\n"," 40%|████      | 4/10 [00:17<00:21,  3.55s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:24<00:25,  5.10s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:25<00:14,  3.58s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:28<00:10,  3.51s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:29<00:05,  2.58s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:36<00:04,  4.02s/it]\u001b[A\n","100%|██████████| 10/10 [00:37<00:00,  3.73s/it]\n","100%|██████████| 80/80 [07:05<00:00,  5.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\n"," 49%|████▉     | 39/80 [02:55<03:27,  5.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:16<02:29, 16.66s/it]\u001b[A\n"," 20%|██        | 2/10 [00:17<00:58,  7.27s/it]\u001b[A\n"," 30%|███       | 3/10 [00:31<01:12, 10.30s/it]\u001b[A\n"," 40%|████      | 4/10 [00:31<00:38,  6.49s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:40<00:35,  7.09s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:40<00:19,  4.88s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:44<00:13,  4.42s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:44<00:06,  3.20s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:52<00:04,  4.61s/it]\u001b[A\n","100%|██████████| 10/10 [00:53<00:00,  5.30s/it]\n"," 99%|█████████▉| 79/80 [06:25<00:04,  4.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:08<01:16,  8.53s/it]\u001b[A\n"," 20%|██        | 2/10 [00:09<00:30,  3.86s/it]\u001b[A\n"," 30%|███       | 3/10 [00:16<00:37,  5.36s/it]\u001b[A\n"," 40%|████      | 4/10 [00:16<00:20,  3.49s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:24<00:25,  5.02s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:25<00:14,  3.51s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:28<00:10,  3.48s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:29<00:05,  2.56s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:36<00:04,  4.02s/it]\u001b[A\n","100%|██████████| 10/10 [00:37<00:00,  3.70s/it]\n","100%|██████████| 80/80 [07:03<00:00,  5.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\n"," 49%|████▉     | 39/80 [02:53<03:21,  4.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:16<02:27, 16.44s/it]\u001b[A\n"," 20%|██        | 2/10 [00:17<00:57,  7.20s/it]\u001b[A\n"," 30%|███       | 3/10 [00:30<01:10, 10.14s/it]\u001b[A\n"," 40%|████      | 4/10 [00:31<00:38,  6.41s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:40<00:36,  7.21s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:40<00:19,  4.96s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:44<00:13,  4.50s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:44<00:06,  3.26s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:52<00:04,  4.53s/it]\u001b[A\n","100%|██████████| 10/10 [00:52<00:00,  5.28s/it]\n"," 99%|█████████▉| 79/80 [06:24<00:04,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:08<01:14,  8.24s/it]\u001b[A\n"," 20%|██        | 2/10 [00:08<00:30,  3.75s/it]\u001b[A\n"," 30%|███       | 3/10 [00:16<00:37,  5.32s/it]\u001b[A\n"," 40%|████      | 4/10 [00:16<00:20,  3.46s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:24<00:25,  5.04s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:25<00:14,  3.54s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:28<00:10,  3.52s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:29<00:05,  2.60s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:36<00:04,  4.02s/it]\u001b[A\n","100%|██████████| 10/10 [00:36<00:00,  3.70s/it]\n","100%|██████████| 80/80 [07:01<00:00,  5.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\n","  1%|▏         | 1/80 [00:08<11:44,  8.92s/it]"]}],"source":["#from part_2 import run\n","\n","all_configs = [\n","            {\n","        \"epochs\": 5,\n","        \"batch_size\": 256,\n","        \"lr\": 1e-2,\n","        \"name\": \"densenet\",\n","        \"model\": \"densenet\",\n","        \"use_wandb\": True,\n","        \"use_amsgrad\": True,\n","        \"use_warm_restarts\": False,\n","        \"use_weighted_loss\": False,\n","        \"data_path\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro\",\n","        \"use_imbalanced_sampler\": True,\n","        #\"use_pretrained\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_1_models/2022-05-17_21-00-07.965720/densenet-final.pth\",\n","    }\n","]\n","\n","for config in all_configs:\n","    run(config)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"quality_models.ipynb","provenance":[],"background_execution":"on","mount_file_id":"1MGy-NJAA8g75KYbGheTqnoQzj9Rl4iB4","authorship_tag":"ABX9TyO57GPpbp8pFMwFpjFpqpEJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9f6c1789a27243dd84c02e5207ea710d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0262e36ab2d448518cf66f27d397273b","IPY_MODEL_7e0a5dd8e7d7412fa845547eb09e014b"],"layout":"IPY_MODEL_fa18ddb6aef748f7898ebd3891cd0528"}},"0262e36ab2d448518cf66f27d397273b":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1193178430e548379f9b3b8e049646cc","placeholder":"​","style":"IPY_MODEL_8fb04864afcc43a18b2487461e3eca31","value":"0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"}},"7e0a5dd8e7d7412fa845547eb09e014b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b14a062cd3174b65824631427f5677e2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9f677114fe3434c937d5526aa65a82f","value":1}},"fa18ddb6aef748f7898ebd3891cd0528":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1193178430e548379f9b3b8e049646cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb04864afcc43a18b2487461e3eca31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b14a062cd3174b65824631427f5677e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9f677114fe3434c937d5526aa65a82f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}