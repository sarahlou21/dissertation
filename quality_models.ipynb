{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11171,"status":"ok","timestamp":1652362408758,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"},"user_tz":-60},"id":"opp4Cld9gck7","outputId":"4dece3f7-9586-41eb-81db-ca1888f29172"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda:0\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f8f71fc6d70>"]},"metadata":{},"execution_count":1}],"source":["!pip install -r /content/drive/MyDrive/Dissertation/requirements.txt -qqq\n","\n","\n","from datetime import datetime\n","import json\n","import torch\n","import torchvision\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import os\n","import numpy as np\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import argmax\n","from tqdm import tqdm\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import seaborn as sn\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",")\n","from torchsampler import ImbalancedDatasetSampler\n","import wandb\n","import numpy as np\n","from os import listdir\n","from os.path import join, isdir\n","from glob import glob\n","import cv2\n","import timeit\n","import timm\n","from sklearn.metrics import confusion_matrix\n","from torchvision.datasets import ImageFolder\n","from collections import Counter\n","from sklearn.utils import class_weight\n","\n","#changing the device to GPU rather than CPU if it is available,\n","# this will decrease model training time.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device {}\".format(device))\n","\n","torch.manual_seed(17)\n"]},{"cell_type":"markdown","metadata":{"id":"-qLIgh8FkgiD"},"source":["Several custom classes were made. The first, ImageResize(), resizes an image using the PIL image resizer set to the bilinear function. A custom class was generated even though Pytorch has its own resizer, as discussed in the following article, https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing there are resizing issues associated with certain libraries such as Pytorch which introduce artefacts or don't provide sufficient antialiasing in the resized images using libraries such as Pytorch.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1652362408759,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"},"user_tz":-60},"id":"b9TywHjhgu2P"},"outputs":[],"source":["class ImageResize(object):\n","        \"\"\"\n","        PIL's resize performs better than pytorch\n","        https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing\n","        \"\"\"\n","\n","        def __init__(self, new_h, new_w):\n","            self.new_h = new_h\n","            self.new_w = new_w\n","\n","        def __call__(self, image):\n","            image = image.resize((self.new_w, self.new_h), resample=Image.BILINEAR)\n","            return image"]},{"cell_type":"markdown","metadata":{"id":"ZeVHonwolOyZ"},"source":["## Network generation\n","\n","Five networks were created using several architectures loaded from the Pytroch library.  Within each model-specific function, the feature extraction layers have been frozen and the final classification layer unfrozen, to allow this final layer to train on our Lesion dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1652362408760,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"},"user_tz":-60},"id":"bxMWZ9CIlK7B"},"outputs":[],"source":["def create_network_densenet():\n","        net = models.densenet161(pretrained=True)\n","        net.classifier = nn.Linear(in_features=2208, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n","\n","\n","def create_network_efficientnet():\n","        net = models.efficientnet_b0(pretrained=True)\n","        net.classifier[1] = nn.Linear(in_features=1280, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n","\n","def create_network_mobilenet():\n","        net = models.mobilenet_v3_small(pretrained=True)\n","        net.classifier[3] = nn.Linear(in_features=1024, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","            param.requires_grad = False\n","        for param in net.classifier.parameters():\n","            param.requires_grad = True\n","\n","        return net\n","\n","\n","def create_network_resnet():\n","        net = models.resnet18(pretrained=True)\n","        net.fc = nn.Linear(in_features=512, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","            param.requires_grad = False\n","        for param in net.fc.parameters():\n","            param.requires_grad = True\n","\n","        return net\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1195,"status":"ok","timestamp":1652362409918,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"},"user_tz":-60},"id":"xDTi6y0Uk7N9"},"outputs":[],"source":["def run(config):  \n","    BASE_PATH = '/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_1_models/'\n","    now = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\")\n","    RESULTS_PATH = os.path.join(BASE_PATH, now)\n","    os.makedirs(RESULTS_PATH, exist_ok=False)\n","\n","    with open(os.path.join(RESULTS_PATH, \"config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=4)\n","\n","    if config.get(\"use_wandb\"):\n","        # this integrates the third-party platform, Weights and Biases,\n","        # with this notebook.\n","        run = wandb.init(\n","            project=\"part_1_skin_lesion\",\n","            entity=\"sarahlouise\",\n","            config=config,\n","        )\n","\n","\n","    # ## Train, validation and test set\n","    #\n","    # The train, validation and test sets were loaded and transformed using the Pytorch functions and amended classes previously discussed.\n","\n","    batch_size = config[\"batch_size\"]\n","    Imagenet_NV = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","    print(\"Configuring datasets\")\n","    trainset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'train'),\n","        transform=transforms.Compose([\n","                            transforms.RandomVerticalFlip(0.5),\n","                            transforms.RandomHorizontalFlip(0.5),\n","                            transforms.RandomApply(nn.ModuleList([transforms.ColorJitter(),\n","                                                            transforms.GaussianBlur(3)]), p=0.1),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","\n","                        ]))\n","\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        trainset,\n","        batch_size=batch_size,\n","        num_workers=2,\n","        sampler=ImbalancedDatasetSampler(trainset, callback_get_label=lambda x: x.targets),\n","    )\n","\n","    valset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'val'),\n","        transform=transforms.Compose([\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","                        ]))\n","\n","    valloader = torch.utils.data.DataLoader(\n","        valset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","\n","    testset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'test'),\n","        transform=transforms.Compose([\n","                                ImageResize(224,224),\n","                                transforms.PILToTensor(),\n","                                transforms.ConvertImageDtype(torch.float),\n","                                transforms.Normalize(*Imagenet_NV),\n","                            ]))\n","\n","    testloader = torch.utils.data.DataLoader(\n","        testset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","    \n","    classes = sorted(os.listdir(os.path.join(config.get(\"data_path\"), 'train')))\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.sklearn.plot_class_proportions(\n","            trainset.targets,\n","            testset.targets,\n","            classes\n","        )\n","\n","\n","    #making logical arguments from the configuration dictionary definied earlier\n","    model_name = config[\"model\"]\n","\n","    if model_name == \"densenet\":\n","        net = create_network_densenet()\n","    elif model_name == \"efficientnet\":\n","        net = create_network_efficientnet()\n","    elif model_name == \"mobilenet\":\n","        net = create_network_mobilenet()\n","    elif model_name == \"resnet\":\n","        net = create_network_resnet()\n","    else:\n","        raise ValueError(\"Model name not supported '{}'\".format(model_name))\n","\n","    print(f\"Moving network to {device}\")\n","    net = net.to(device)\n","\n","    # ## Training configurations\n","    \n","    # defining the training configurations\n","    no_of_epochs = config[\"epochs\"]\n","    \n","    # calculating class weights\n","    # labels = np.array(trainset.targets)\n","    # class_weights = class_weight.compute_class_weight(\n","    #     class_weight='balanced',\n","    #     classes=np.unique(labels),\n","    #     y=labels\n","    # )\n","    # class_weights = torch.tensor(\n","    #     class_weights,\n","    #     dtype=torch.float\n","    # )\n","\n","    # criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    optimizer = optim.Adam(\n","        net.parameters(),\n","        lr=config[\"lr\"],\n","        amsgrad=config[\"use_amsgrad\"]\n","    )\n","\n","    if config[\"use_warm_restarts\"]:\n","        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","            optimizer,\n","            T_0=10,\n","            T_mult=1,\n","        )\n","    else:\n","        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","            optimizer,\n","            round((len(trainset)/batch_size)*no_of_epochs)\n","        )\n","\n","\n","    # ##  Model training\n","    #\n","    # This section included the training for the model. The network was set to train, a mini-batch was passed through the model and then the loss was then backpropagated. Each trained model was also automatically saved.\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.watch(net)\n","\n","\n","    if not config.get(\"use_pretrained\"):\n","        print(\"Starting training\")\n","        for epoch in range(no_of_epochs):  # loop over the dataset multiple times\n","            print(\"Epoch {} of {}\".format(epoch+1, no_of_epochs))\n","            epoch_running_loss = []\n","            epoch_val_metric = []\n","            for i, data in tqdm(enumerate(trainloader, 1), total=len(trainloader)):\n","                current_step = (len(trainloader) * epoch) + i\n","                # get the inputs; data is a list of [inputs, labels]\n","                net.train()\n","                inputs, labels = data\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward + backward\n","                outputs = net(inputs)\n","                # outputs = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                # saving loss statistics\n","                loss_val = loss.item()\n","                if config.get(\"use_wandb\"):\n","                    wandb.log(\n","                        {\n","                            \"train/bce_logits_loss\": loss_val,\n","                            \"train/lr\": scheduler.get_last_lr()[0],\n","                            \"train/custom_step\": (len(trainloader) * epoch) + i,\n","                        }\n","                    )\n","\n","                # adding in valiation data testing\n","                if i % 40 == 0:\n","                    model_name = \"{name}-epoch-{epoch}-step-{step}.pth\".format(\n","                        name=config[\"name\"],\n","                        epoch=epoch,\n","                        step=i,\n","                    )\n","                    model_path = os.path.join(RESULTS_PATH, model_name)\n","                    torch.save(net.state_dict(), model_path)\n","\n","                    predicted = []\n","                    actual = []\n","\n","                    print(\"Running validation\")\n","                    net.eval()\n","                    with torch.no_grad():\n","                        for i, data in enumerate(tqdm(valloader, total=len(valloader))):\n","                            inputs, labels = data\n","                            inputs = inputs.to(device)\n","                            labels = labels.to(device)\n","\n","                            outputs = net(inputs)\n","                            loss = criterion(outputs, labels)\n","                            if config.get(\"use_wandb\"):\n","                                wandb.log({\n","                                    \"val/loss\": loss,\n","                                    \"val/custom_step\": current_step,\n","                                })\n","\n","                            # collect the correct predictions for each class\n","                            actual.extend(labels.detach().cpu().numpy())\n","                            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","\n","                        mb_acc = accuracy_score(actual, predicted)\n","                        precision = precision_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        recall = recall_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        metric_report = classification_report(\n","                            np.array(actual),\n","                            np.array(predicted),\n","                            output_dict=True,\n","                            zero_division=1,\n","                        )\n","                        metric_report = {\n","                            f\"val_classes/{k}\": v\n","                            for k, v in metric_report.items()\n","                        }\n","                        metric_report.update({\n","                            \"val_classes/custom_step\": current_step,\n","                        })\n","\n","                        if config.get(\"use_wandb\"):\n","                            wandb.log({\n","                                \"val/accuracy\": mb_acc,\n","                                \"val/precision\": precision,\n","                                \"val/recall\": recall,\n","                                \"val/f1\": f1,\n","                                \"val/custom_step\": current_step\n","                            })\n","                            wandb.log(metric_report)\n","\n","\n","\n","        print('Finished Training')\n","        # saving each trained model\n","        model_name = '{}-final.pth'.format(\n","            config[\"name\"]\n","        )\n","        model_path = os.path.join(RESULTS_PATH, model_name)\n","        torch.save(net.state_dict(), model_path)\n","\n","        print(\"Final model saved\")\n","\n","    # ##  Model testing\n","    #\n","    # The model was set to evaluate mode for testing. The predicted and actual results were saved, and used for generating a confusion matrix and metrics.\n","    #\n","    # A confusion matrix was generated, showing the number of true positives and negatives, and false positives and negatives. Additionally, the metrics were calculated, including the accuracy score, precision and recall. The corresponding graphs were created using Weights and Biases.\n","\n","\n","    if config.get(\"use_pretrained\"):\n","        net.load_state_dict(\n","            torch.load(\n","                config.get(\"use_pretrained\")\n","            )\n","        )\n","\n","    predicted = []\n","    actual = []\n","\n","    print(\"Testing model\")\n","    # again no gradients needed\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(tqdm(testloader, total=len(testloader))):\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = net(inputs)\n","            actual.extend(labels.detach().cpu().numpy())\n","            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","\n","    print(\"Calculating performance metrics\")\n","    model_accuracy = accuracy_score(actual, predicted)\n","    model_precision = precision = precision_score(actual, predicted,\n","                                                average=\"macro\", zero_division = 1)\n","    model_recall = recall_score(actual, predicted,\n","                                average=\"macro\", zero_division = 1)\n","    model_f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","    metric_report = classification_report(\n","        np.array(actual),\n","        np.array(predicted),\n","        output_dict=True,\n","        zero_division=1,\n","    )\n","    metric_report = {\n","        f\"test_classes/{k}\": v\n","        for k, v in metric_report.items()\n","    }\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.log({\"test/accuracy\": model_accuracy})\n","        wandb.log({\"test/precision\": model_precision})\n","        wandb.log({\"test/recall\": model_recall})\n","        wandb.log({\"test/f1\": model_f1})\n","        wandb.sklearn.plot_confusion_matrix(actual, predicted, classes)\n","    else:\n","        print(\"test/accuracy\", model_accuracy)\n","        print(\"test/precision\", model_precision)\n","        print(\"test/recall\", model_recall)\n","        print(\"test/f1\", model_f1)\n","\n","    plot_save_path = os.path.join(RESULTS_PATH, '{}_confusion_matrix.png'.format(\n","        config[\"name\"]\n","    ))\n","    print(\"Plotting confusion matrix in {}\".format(plot_save_path))\n","\n","    print(\"Calculating\")\n","    conf_matrix = confusion_matrix(y_true=actual, y_pred=predicted)\n","    print(\"Done\")\n","    fig =plt.figure(figsize=(20, 20))\n","    ax = fig.add_subplot(1,1,1)\n","    ax.matshow(conf_matrix, cmap=plt.cm.Reds)\n","\n","\n","    plt.xlabel('Predictions')\n","    plt.ylabel('Actuals')\n","    plt.title('Confusion Matrix')\n","    plt.savefig(plot_save_path)\n","    # plt.show()\n","\n","    if config.get(\"use_wandb\"):\n","        run.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xjSeEY5BUdey","outputId":"e4d068d0-c9a6-4dd7-cef6-1c60e632b936"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msarahlouise\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.16"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220512_133331-2gfa57pu</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion/runs/2gfa57pu\" target=\"_blank\">pretty-pond-40</a></strong> to <a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Configuring datasets\n","Moving network to cuda:0\n","Starting training\n","Epoch 1 of 5\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▉     | 39/80 [04:43<05:19,  7.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:23<03:32, 23.66s/it]\u001b[A\n"," 20%|██        | 2/10 [00:25<01:26, 10.79s/it]\u001b[A\n"," 30%|███       | 3/10 [00:43<01:39, 14.18s/it]\u001b[A\n"," 40%|████      | 4/10 [00:45<00:55,  9.28s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:58<00:52, 10.47s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:59<00:29,  7.47s/it]\u001b[A\n"," 70%|███████   | 7/10 [01:05<00:20,  6.87s/it]\u001b[A\n"," 80%|████████  | 8/10 [01:06<00:10,  5.21s/it]\u001b[A\n"," 90%|█████████ | 9/10 [01:16<00:06,  6.63s/it]\u001b[A\n","100%|██████████| 10/10 [01:18<00:00,  7.83s/it]\n"," 99%|█████████▉| 79/80 [10:09<00:06,  6.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:12<01:53, 12.65s/it]\u001b[A\n"," 20%|██        | 2/10 [00:14<00:49,  6.19s/it]\u001b[A\n"," 30%|███       | 3/10 [00:23<00:52,  7.49s/it]\u001b[A\n"," 40%|████      | 4/10 [00:25<00:31,  5.19s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:34<00:33,  6.78s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:36<00:20,  5.04s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:41<00:15,  5.24s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:43<00:08,  4.11s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:53<00:05,  5.85s/it]\u001b[A\n","100%|██████████| 10/10 [00:54<00:00,  5.49s/it]\n","100%|██████████| 80/80 [11:05<00:00,  8.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\n"," 49%|████▉     | 39/80 [04:49<05:46,  8.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:22<03:24, 22.78s/it]\u001b[A\n"," 20%|██        | 2/10 [00:24<01:23, 10.44s/it]\u001b[A\n"," 30%|███       | 3/10 [00:41<01:34, 13.53s/it]\u001b[A\n"," 40%|████      | 4/10 [00:43<00:53,  8.89s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:56<00:51, 10.32s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:58<00:29,  7.37s/it]\u001b[A\n"," 70%|███████   | 7/10 [01:03<00:20,  6.80s/it]\u001b[A\n"," 80%|████████  | 8/10 [01:05<00:10,  5.18s/it]\u001b[A\n"," 90%|█████████ | 9/10 [01:15<00:06,  6.61s/it]\u001b[A\n","100%|██████████| 10/10 [01:16<00:00,  7.67s/it]\n"," 99%|█████████▉| 79/80 [10:36<00:07,  7.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:12<01:54, 12.78s/it]\u001b[A\n"," 20%|██        | 2/10 [00:14<00:49,  6.24s/it]\u001b[A\n"," 30%|███       | 3/10 [00:23<00:53,  7.59s/it]\u001b[A\n"," 40%|████      | 4/10 [00:25<00:31,  5.25s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:35<00:34,  6.87s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:36<00:20,  5.11s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:42<00:15,  5.30s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:44<00:08,  4.15s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:53<00:05,  5.85s/it]\u001b[A\n","100%|██████████| 10/10 [00:55<00:00,  5.53s/it]\n","100%|██████████| 80/80 [11:33<00:00,  8.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\n"," 49%|████▉     | 39/80 [05:00<05:41,  8.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:22<03:18, 22.09s/it]\u001b[A\n"," 20%|██        | 2/10 [00:23<01:21, 10.15s/it]\u001b[A\n"," 30%|███       | 3/10 [00:40<01:32, 13.28s/it]\u001b[A\n"," 40%|████      | 4/10 [00:42<00:52,  8.73s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:54<00:48,  9.75s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:55<00:28,  7.00s/it]\u001b[A\n"," 70%|███████   | 7/10 [01:01<00:19,  6.59s/it]\u001b[A\n"," 80%|████████  | 8/10 [01:03<00:10,  5.03s/it]\u001b[A\n"," 90%|█████████ | 9/10 [01:12<00:06,  6.38s/it]\u001b[A\n","100%|██████████| 10/10 [01:14<00:00,  7.42s/it]\n"," 99%|█████████▉| 79/80 [10:43<00:06,  6.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Running validation\n"]},{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:13<01:57, 13.04s/it]\u001b[A\n"," 20%|██        | 2/10 [00:14<00:50,  6.34s/it]\u001b[A\n"," 30%|███       | 3/10 [00:24<00:53,  7.70s/it]\u001b[A\n"," 40%|████      | 4/10 [00:25<00:31,  5.32s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:35<00:34,  6.93s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:37<00:20,  5.14s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:42<00:15,  5.32s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:44<00:08,  4.17s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:54<00:05,  5.86s/it]\u001b[A\n","100%|██████████| 10/10 [00:55<00:00,  5.57s/it]\n","100%|██████████| 80/80 [11:40<00:00,  8.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\n","  2%|▎         | 2/80 [00:18<10:34,  8.14s/it]"]}],"source":["#from part_2 import run\n","\n","all_configs = [\n","            {\n","        \"epochs\": 5,\n","        \"batch_size\": 256,\n","        \"lr\": 1e-2,\n","        \"name\": \"densenet\",\n","        \"model\": \"densenet\",\n","        \"use_wandb\": True,\n","        \"use_amsgrad\": True,\n","        \"use_warm_restarts\": False,\n","        \"data_path\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_Split_val\",\n","        #\"use_pretrained\": \"./densenet-final.pth\",\n","    },\n","]\n","\n","for config in all_configs:\n","    run(config)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"quality_models.ipynb","provenance":[],"mount_file_id":"1MGy-NJAA8g75KYbGheTqnoQzj9Rl4iB4","authorship_tag":"ABX9TyOubtr9vx6G7OeFwwE7BJjN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}