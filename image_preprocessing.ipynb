{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"image_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1TWWiuoqIKT4Bwa627oBYTkr7rHFkZJEc","authorship_tag":"ABX9TyNr4BDUpe8Mo+QaszJVEMP5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ALYJ-BhE4i8A"},"outputs":[],"source":["# hair removal- commonly done in alot of peoples approaches however I cant see it within Gesserts work \n","\n","# this repo has both hair rmeoval code and colour consistancy https://github.com/ThiruRJST/Melanoma_Classification \n","import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","from Preprocessing_Utils.Serializer import  Serialize_Write\n","\n","\n","def hair_remove(image):\n","    # convert image to grayScale\n","    grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","\n","    # kernel for morphologyEx\n","    kernel = cv2.getStructuringElement(1, (17, 17))\n","\n","    # apply MORPH_BLACKHAT to grayScale image\n","    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","\n","    # apply thresholding to blackhat\n","    _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n","\n","    # inpaint with original image and threshold image\n","    final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n","\n","    return final_image\n","\n","\n","def shade_of_gray_cc(img, power=6, gamma=2.2):\n","    \"\"\"\n","    img (numpy array): the original image with format of (h, w, c)\n","    power (int): the degree of norm, 6 is used in reference paper\n","    gamma (float): the value of gamma correction, 2.2 is used in reference paper\n","    \"\"\"\n","    img_dtype = img.dtype\n","\n","    if gamma is not None:\n","        img = img.astype('uint8')\n","        look_up_table = np.ones((256, 1), dtype='uint8') * 0\n","        for i in range(256):\n","            look_up_table[i][0] = 255 * pow(i / 255, 1 / gamma)\n","        img = cv2.LUT(img, look_up_table)\n","\n","    img = img.astype('float32')\n","    img_power = np.power(img, power)\n","    rgb_vec = np.power(np.mean(img_power, (0, 1)), 1 / power)\n","    rgb_norm = np.sqrt(np.sum(np.power(rgb_vec, 2.0)))\n","    rgb_vec = rgb_vec / rgb_norm\n","    rgb_vec = 1 / (rgb_vec * np.sqrt(3))\n","    img = np.multiply(img, rgb_vec)\n","\n","    # Andrew Anikin suggestion\n","    img = np.clip(img, a_min=0, a_max=255)\n","\n","    return img.astype(img_dtype)\n"]},{"cell_type":"code","source":["# cropping out black border unfortunately Gessert uses Matlab for doing this https://github.com/ngessert/isic2019/blob/2394aaf60e7e070a5d0197a381426a1259a5f1bb/Matlab/adjust_2019.m \n","\n","# \"We binarize the images with a very low threshold, such that the entire dermoscopy field of view is set to 1. Then, we find the center of mass and the major and minor axis of an ellipse that has the same second central moments as the inner area. Based on these values we derive a rectangular bounding box for cropping that covers the\n","# relevant field of view. We automatically determin the necessecity for cropping based on a heuristic that tests whether the mean intensity inside the bounding box is substantially different from the mean intensity outside of the bounding box.\"\n","\n","# ALTERNATIVELY THIS METHOD ON stack OVERFLOW https://stackoverflow.com/questions/61986407/crop-x-ray-image-to-remove-black-background \n","\n","# could develop this by cropping the image more to omit more of the black egdes or look into inpainting https://github.com/fitushar/Skin-lesion-Segmentation-using-grabcut/blob/master/Skin%20lesion%20segmentation%20using%20grabcut%20in%20HSV%20color%20space.pdf \n","\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","\n","# load image as grayscale\n","img = cv2.imread('/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_Split_val/train/AK/ISIC_0053944.jpg')\n","\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","# threshold \n","thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n","hh, ww = thresh.shape\n","\n","# make bottom 2 rows black where they are white the full width of the image\n","thresh[hh-3:hh, 0:ww] = 0\n","\n","# get bounds of white pixels\n","white = np.where(thresh==255)\n","xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n","print(xmin,xmax,ymin,ymax)\n","\n","# crop the image at the bounds adding back the two blackened rows at the bottom\n","crop = img[ymin:ymax+3, xmin:xmax]\n","\n","# save resulting masked image\n","cv2.imwrite('lesion_thresh.jpg', thresh)\n","cv2.imwrite('lesion_crop.jpg', crop)\n","\n","# display result\n","cv2_imshow(img)\n","cv2_imshow(thresh)\n","cv2_imshow(crop)\n"],"metadata":{"id":"MdDUq0oS7gqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Augmentation \n","\n","# CURRENT \n","transform=transforms.Compose([\n","                            transforms.RandomVerticalFlip(0.5),\n","                            transforms.RandomHorizontalFlip(0.5),\n","                            transforms.RandomApply(nn.ModuleList([transforms.ColorJitter(),\n","                                                            transforms.GaussianBlur(3)]), p=0.1),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n",")\n","\n","# From Gessert however this doesnt match up with the paper??? The paper also mentions shear and other augmentation \n","\n","transform = transforms.Compose([\n","                    cropping,\n","                    transforms.RandomHorizontalFlip(),\n","                    transforms.RandomVerticalFlip(),\n","                    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(torch.from_numpy(self.setMean).float(),torch.from_numpy(np.array([1.,1.,1.])).float())\n","                    ])   \n"],"metadata":{"id":"XlRSakw0jop6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gessert also inclues CutOut with is added into the transforms as (Cutout_v0(n_holes=1,length=self.mdlParams['cutout']))\n","\n","class Cutout_v0(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        img = np.array(img)\n","        #print(img.shape)\n","        h = img.shape[0]\n","        w = img.shape[1]\n","\n","        mask = np.ones((h, w), np.uint8)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        #mask = torch.from_numpy(mask)\n","        #mask = mask.expand_as(img)\n","        img = img * np.expand_dims(mask,axis=2)\n","        img = Image.fromarray(img)\n","        return img    "],"metadata":{"id":"yZdc7uvnly9q"},"execution_count":null,"outputs":[]}]}