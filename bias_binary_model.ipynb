{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"id":"opp4Cld9gck7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656440792558,"user_tz":-60,"elapsed":11518,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}},"outputId":"cd207fe9-05d5-44e4-e4b2-6c271ad45a58"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.34.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython) (0.18.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython) (0.1.3)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (3.0.30)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.5)\n","Using device cuda:0\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6b61971350>"]},"metadata":{},"execution_count":22}],"source":["\n","!pip install -U ipython\n","!pip install -r /content/drive/MyDrive/Dissertation/requirements.txt -qqq\n","!pip install ipdb -qqq\n","\n","\n","from datetime import datetime\n","import json\n","import torch\n","import torchvision\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import os\n","import numpy as np\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import argmax\n","from tqdm import tqdm\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import seaborn as sn\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score, classification_report, balanced_accuracy_score\n",")\n","from torchsampler import ImbalancedDatasetSampler\n","import wandb\n","import numpy as np\n","from os import listdir\n","from os.path import join, isdir\n","from glob import glob\n","import cv2\n","import timeit\n","import timm\n","from sklearn.metrics import confusion_matrix\n","from torchvision.datasets import ImageFolder\n","from collections import Counter\n","from sklearn.utils import class_weight\n","from torchvision.transforms import AutoAugment\n","from operator import itemgetter\n","import random\n","\n","from torchvision.transforms.autoaugment import AutoAugmentPolicy\n","\n","#changing the device to GPU rather than CPU if it is available,\n","# this will decrease model training time.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device {}\".format(device))\n","\n","torch.manual_seed(17)"]},{"cell_type":"code","source":["class ImageResize(object):\n","        \"\"\"\n","        PIL's resize performs better than pytorch\n","        https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing\n","        \"\"\"\n","\n","        def __init__(self, new_h, new_w):\n","            self.new_h = new_h\n","            self.new_w = new_w\n","\n","        def __call__(self, image):\n","            image = image.resize((self.new_w, self.new_h), resample=Image.BILINEAR)\n","            return image"],"metadata":{"id":"OqY5UiOBX5sG","executionInfo":{"status":"ok","timestamp":1656440792559,"user_tz":-60,"elapsed":59,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class BinaryClassBasedCustomImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, male_count, female_count, path_meta_data):\n","        # mess with dataset\n","        total_meta_data = pd.read_csv(path_meta_data)\n","\n","        total_meta_data[\"class_id\"] = [None] * len(total_meta_data)\n","        for img_path, class_id in zip(dataset.imgs, dataset.targets):\n","            image_id = img_path[0].split(\"/\")[-1].split(\".\")[0]\n","            total_meta_data.loc[total_meta_data[\"image\"] == image_id, \"class_id\"] = class_id\n","\n","        train_metadata = total_meta_data.groupby(\"rand_split\").get_group(\"train\")\n","\n","        sex_group = train_metadata.groupby(\"sex\")\n","        male_df = sex_group.get_group(\"male\").sample(n=male_count)\n","        female_df = sex_group.get_group(\"female\").sample(n=female_count)\n","\n","        filtered_dataset_names = pd.concat([male_df, female_df]).get([\n","            \"image_path\", \"class_id\"\n","        ]).values\n","\n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)"],"metadata":{"id":"XSkBuT1bBuSh","executionInfo":{"status":"ok","timestamp":1656440792560,"user_tz":-60,"elapsed":58,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZeVHonwolOyZ"},"source":["## Network generation\n","\n","Five networks were created using several architectures loaded from the Pytroch library.  Within each model-specific function, the feature extraction layers have been frozen and the final classification layer unfrozen, to allow this final layer to train on our Lesion dataset."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"bxMWZ9CIlK7B","executionInfo":{"status":"ok","timestamp":1656440792561,"user_tz":-60,"elapsed":58,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["def create_network_densenet():\n","        net = models.densenet161(pretrained=True)\n","        net.classifier = nn.Linear(in_features=2208, out_features=2, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n"]},{"cell_type":"code","source":["class ImageFolderWithPaths(ImageFolder):\n","\n","    def __getitem__(self, index: int):\n","        path, target = self.samples[index]\n","        sample = self.loader(path)\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return sample, target, path"],"metadata":{"id":"FYQuy_tlq9BV","executionInfo":{"status":"ok","timestamp":1656440792562,"user_tz":-60,"elapsed":58,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"id":"xDTi6y0Uk7N9","executionInfo":{"status":"ok","timestamp":1656440792563,"user_tz":-60,"elapsed":59,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["def run(config):  \n","    torch.manual_seed(17)\n","    BASE_PATH = '/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_2_models/'\n","    now = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\")\n","    RESULTS_PATH = os.path.join(BASE_PATH, now)\n","    os.makedirs(RESULTS_PATH, exist_ok=False)\n","\n","    with open(os.path.join(RESULTS_PATH, \"config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=4)\n","\n","    if config.get(\"use_wandb\"):\n","        # this integrates the third-party platform, Weights and Biases,\n","        # with this notebook.\n","        run = wandb.init(\n","            project=\"part_2_skin_lesion\",\n","            entity=\"sarahlouise\",\n","            config=config,\n","        )\n","\n","    batch_size = config[\"batch_size\"]\n","    Imagenet_NV = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","    print(\"Configuring datasets\")\n","    trainset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'train'),\n","        transform=transforms.Compose([\n","                            transforms.RandomVerticalFlip(0.5),\n","                            transforms.RandomHorizontalFlip(0.5),\n","                            transforms.RandomApply(nn.ModuleList([transforms.RandomRotation(degrees=(0, 360))]), p=0.2),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","                        ])\n","        )\n","    \n","    imbalanced_sampler_choice = [\n","        config.get(\"use_binary_gender_custom_imbalanced_sampler\"),\n","        config.get(\"use_imbalanced_sampler\"),\n","    ]\n","    if imbalanced_sampler_choice.count(True) > 1:\n","        raise ValueError(\"Pick one sampler only\")\n","\n","    elif config.get(\"use_binary_gender_custom_imbalanced_sampler\"):\n","        sampler = BinaryClassBasedCustomImbalancedDatasetSampler(\n","            dataset=trainset,\n","            callback_get_label=lambda x: x.targets,\n","            male_count=config.get(\"male_count\"),\n","            female_count=config.get(\"female_count\"),\n","            path_meta_data=config.get(\"path_meta_data\"),\n","        )\n","        shuffle = False\n","    elif config.get(\"use_imbalanced_sampler\"):\n","        sampler = ImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets,\n","        )\n","        shuffle = False\n","    else:\n","        sampler = None\n","        shuffle = True\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        trainset,\n","        batch_size=batch_size,\n","        num_workers=2,\n","        sampler=ImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets\n","    ))\n","\n","    valset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'val'),\n","        transform=transforms.Compose([\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","                        ]))\n","\n","    valloader = torch.utils.data.DataLoader(\n","        valset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","\n","    testset = ImageFolderWithPaths(\n","        os.path.join(config.get(\"data_path\"), 'test'),\n","        transform=transforms.Compose([\n","                                ImageResize(224,224),\n","                                transforms.PILToTensor(),\n","                                transforms.ConvertImageDtype(torch.float),\n","                                transforms.Normalize(*Imagenet_NV),\n","                            ]))\n","\n","    testloader = torch.utils.data.DataLoader(\n","        testset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","    \n","    classes = sorted(os.listdir(os.path.join(config.get(\"data_path\"), 'train')))\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.sklearn.plot_class_proportions(\n","            trainset.targets,\n","            testset.targets,\n","            classes\n","        )\n","\n","\n","    net = create_network_densenet()\n","    print(f\"Moving network to {device}\")\n","    net = net.to(device)\n","\n","    # ## Training configurations\n","    \n","    # defining the training configurations\n","    no_of_epochs = config[\"epochs\"]\n","    \n","    # calculating class weights\n","\n","    labels = np.array(trainset.targets)\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    optimizer = optim.Adam(\n","        net.parameters(),\n","        lr=config[\"lr\"],\n","        amsgrad=config[\"use_amsgrad\"]\n","    )\n","\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer,\n","        round((len(trainset)/batch_size)*no_of_epochs)\n","        )\n","\n","\n","    # ##  Model training\n","    #\n","    # This section included the training for the model. The network was set to train, a mini-batch was passed through the model and then the loss was then backpropagated. Each trained model was also automatically saved.\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.watch(net)\n","\n","    if not config.get(\"use_pretrained\"):\n","        print(\"Starting training\")\n","        for epoch in range(no_of_epochs):  # loop over the dataset multiple times\n","            print(\"Epoch {} of {}\".format(epoch+1, no_of_epochs))\n","            epoch_running_loss = []\n","            epoch_val_metric = []\n","            for i, data in tqdm(enumerate(trainloader, 1), total=len(trainloader)):\n","                current_step = (len(trainloader) * epoch) + i\n","                \n","                # get the inputs; data is a list of [inputs, labels]\n","                net.train()\n","                \n","                inputs, labels = data\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                \n","                # forward + backward\n","                outputs = net(inputs)\n","\n","                \n","                # outputs = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","                \n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                # saving loss statistics\n","                loss_val = loss.item()\n","                if config.get(\"use_wandb\"):\n","                    wandb.log(\n","                        {\n","                            \"train/bce_logits_loss\": loss_val,\n","                            \"train/lr\": scheduler.get_last_lr()[0],\n","                            \"train/custom_step\": (len(trainloader) * epoch) + i,\n","                        }\n","                    )\n","\n","                # adding in valiation data testing\n","                if i % round(40*config.get(\"percent_keep\", 1)) == 0:\n","                    model_name = \"{name}-epoch-{epoch}-step-{step}.pth\".format(\n","                        name=config[\"name\"],\n","                        epoch=epoch,\n","                        step=i,\n","                    )\n","                    model_path = os.path.join(RESULTS_PATH, model_name)\n","                    # Commenting out to save space\n","                    # torch.save(net.state_dict(), model_path)\n","\n","                    predicted = []\n","                    actual = []\n","\n","                    print(\"Running validation\")\n","                    net.eval()\n","                    with torch.no_grad():\n","                        for i, data in enumerate(tqdm(valloader, total=len(valloader))):\n","                            inputs, labels = data\n","                            inputs = inputs.to(device)\n","                            labels = labels.to(device)\n","\n","                            outputs = net(inputs)\n","                            loss = criterion(outputs, labels)\n","                            if config.get(\"use_wandb\"):\n","                                wandb.log({\n","                                    \"val/loss\": loss,\n","                                    \"val/custom_step\": current_step,\n","                                })\n","\n","                            # collect the correct predictions for each class\n","                            actual.extend(labels.detach().cpu().numpy())\n","                            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","\n","                        mb_acc = accuracy_score(actual, predicted)\n","                        precision = precision_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        recall = recall_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        bal_acc = balanced_accuracy_score(actual, predicted)\n","                        metric_report = classification_report(\n","                            np.array(actual),\n","                            np.array(predicted),\n","                            output_dict=True,\n","                            zero_division=1,\n","                        )\n","                        metric_report = {\n","                            f\"val_classes/{k}\": v\n","                            for k, v in metric_report.items()\n","                        }\n","                        metric_report.update({\n","                            \"val_classes/custom_step\": current_step,\n","                        })\n","\n","                        if config.get(\"use_wandb\"):\n","                            wandb.log({\n","                                \"val/accuracy\": mb_acc,\n","                                \"val/precision\": precision,\n","                                \"val/recall\": recall,\n","                                \"val/f1\": f1,\n","                                \"val/balanced_acc\": bal_acc,\n","                                \"val/custom_step\": current_step\n","                            })\n","                            wandb.log(metric_report)\n","\n","\n","\n","        print('Finished Training')\n","        # saving each trained model\n","        model_name = '{}-final.pth'.format(\n","            config[\"name\"]\n","        )\n","        model_path = os.path.join(RESULTS_PATH, model_name)\n","        torch.save(net.state_dict(), model_path)\n","\n","        print(\"Final model saved\")\n","\n","    # ##  Model testing\n","    #\n","    # The model was set to evaluate mode for testing. The predicted and actual results were saved, and used for generating a confusion matrix and metrics.\n","    #\n","    # A confusion matrix was generated, showing the number of true positives and negatives, and false positives and negatives. Additionally, the metrics were calculated, including the accuracy score, precision and recall. The corresponding graphs were created using Weights and Biases.\n","\n","\n","    if config.get(\"use_pretrained\"):\n","        net.load_state_dict(\n","            torch.load(\n","                config.get(\"use_pretrained\")\n","            )\n","        )\n","\n","    predicted = []\n","    actual = []\n","    image_paths = []\n","    \n","    print(\"Testing model\")\n","    # again no gradients needed\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(tqdm(testloader, total=len(testloader))):\n","            inputs, labels, paths = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = net(inputs)\n","            actual.extend(labels.detach().cpu().numpy())\n","            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","            image_paths.extend(paths)\n","\n","\n","    results = pd.DataFrame.from_dict(\n","        {\n","            \"predicted\": predicted,\n","            \"actual\": actual,\n","            \"image_path\": image_paths\n","        }\n","    )\n","    results.to_csv(os.path.join(RESULTS_PATH, \"each_image_predictions.csv\"))\n","\n","\n","    print(\"Calculating performance metrics\")\n","    model_accuracy = accuracy_score(actual, predicted)\n","    model_precision = precision = precision_score(actual, predicted,\n","                                                average=\"macro\", zero_division = 1)\n","    model_recall = recall_score(actual, predicted,\n","                                average=\"macro\", zero_division = 1)\n","    model_f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","    model_bal_acc = balanced_accuracy_score(actual, predicted)\n","    metric_report = classification_report(\n","        np.array(actual),\n","        np.array(predicted),\n","        output_dict=True,\n","        zero_division=1,\n","    )\n","    metric_report = {\n","        f\"test_classes/{k}\": v\n","        for k, v in metric_report.items()\n","    }\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.log({\"test/accuracy\": model_accuracy})\n","        wandb.log({\"test/precision\": model_precision})\n","        wandb.log({\"test/recall\": model_recall})\n","        wandb.log({\"test/f1\": model_f1})\n","        wandb.log({\"test/bal_acc\": model_bal_acc})\n","        wandb.sklearn.plot_confusion_matrix(actual, predicted, classes, normalize=\"true\")\n","        wandb.log(metric_report)\n","    else:\n","        print(\"test/accuracy\", model_accuracy)\n","        print(\"test/precision\", model_precision)\n","        print(\"test/recall\", model_recall)\n","        print(\"test/f1\", model_f1)\n","        print(\"test/bal_acc\", model_bal_acc)\n","\n","    plot_save_path = os.path.join(RESULTS_PATH, '{}_confusion_matrix.png'.format(\n","        config[\"name\"]\n","    ))\n","    print(\"Plotting confusion matrix in {}\".format(plot_save_path))\n","\n","    print(\"Calculating\")\n","    conf_matrix = confusion_matrix(y_true=actual, y_pred=predicted, normalize=\"true\")\n","    print(\"Done\")\n","    fig =plt.figure(figsize=(20, 20))\n","    ax = fig.add_subplot(1,1,1)\n","    ax.matshow(conf_matrix, cmap=plt.cm.Reds)\n","\n","\n","    plt.xlabel('Predictions')\n","    plt.ylabel('Actuals')\n","    plt.title('Confusion Matrix')\n","    plt.savefig(plot_save_path)\n","    # plt.show()\n","\n","    if config.get(\"use_wandb\"):\n","        run.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjSeEY5BUdey","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"51d2534b-b961-410c-8b78-35dde14a438e"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msarahlouise\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.19"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220628_182631-337lcf5q</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sarahlouise/part_2_skin_lesion/runs/337lcf5q\" target=\"_blank\">proud-field-17</a></strong> to <a href=\"https://wandb.ai/sarahlouise/part_2_skin_lesion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Configuring datasets\n","Moving network to cuda:0\n","Starting training\n","Epoch 1 of 5\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/47 [00:00<?, ?it/s]"]}],"source":["#from part_2 import run\n","\n","conf = {\n","    \"epochs\": 5,\n","    \"batch_size\": 256,\n","    \"lr\": 1e-2,\n","    \"name\": \"densenet\",\n","    \"use_wandb\": True,\n","    \"use_amsgrad\": True,\n","    \"data_path\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro_binary\",\n","    \"use_imbalanced_sampler\": False,\n","    \"use_binary_gender_custom_imbalanced_sampler\": True,\n","    \"male_count\": 6000,\n","    \"female_count\": 6000,\n","    \"path_meta_data\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/mel_vs_nonmel.csv\",\n","}\n","run(conf)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"bias_binary_model.ipynb","provenance":[],"background_execution":"on","mount_file_id":"1mChFP20JRc1SVMpBgK1zOHEwY33KIvHm","authorship_tag":"ABX9TyN8zaSTQv2CytfYeGBmVd8Z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}