{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"image_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1TWWiuoqIKT4Bwa627oBYTkr7rHFkZJEc","authorship_tag":"ABX9TyNDtVqS14VtzdoJ3tVtdXBR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Order in which Gessert did it \n","\n","1. Preprocessing\n","- cropping the black edges out\n","- shades of grey consistency method\n","\n","\n","2. Augmentation\n","- augmentation (flipping, contrast etc)/ they also said they tried using AutoAugment policy\n","- CutOut (1 hole and size of 16)\n","\n","Things I wanrt to add in - hair removal (within preprocessing )"],"metadata":{"id":"XgXrt-82MJIY"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","#from Preprocessing_Utils.Serializer import  Serialize_Write\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","import json\n","\n","\n"],"metadata":{"id":"AKkbK6b4J-bH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALYJ-BhE4i8A"},"outputs":[],"source":["# hair removal- commonly done in alot of peoples approaches however I cant see it within Gesserts work \n","\n","# this repo has both hair rmeoval code and colour consistancy https://github.com/ThiruRJST/Melanoma_Classification \n","\n","class HairRemoval(object):\n","        \"\"\"\n","        Hair removal code\n","        https://github.com/ThiruRJST/Melanoma_Classification \n","        \"\"\"\n","\n","        def __call__(self, image):\n","            # convert image to grayScale\n","            grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","\n","            # kernel for morphologyEx\n","            kernel = cv2.getStructuringElement(1, (17, 17))\n","\n","            # apply MORPH_BLACKHAT to grayScale image\n","            blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","\n","            # apply thresholding to blackhat\n","            _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n","\n","            # inpaint with original image and threshold image\n","            final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n","\n","            return final_image\n","\n","\n","class ShadesOfGrey(object):\n","        \"\"\"\n","        Code from https://github.com/ThiruRJST/Melanoma_Classification\n","        imgage (numpy array): the original image with format of (h, w, c)\n","        power (int): the degree of norm, 6 is used in reference paper\n","        gamma (float): the value of gamma correction, 2.2 is used in reference paper\n","        \"\"\"\n","\n","        def __init__(self, power=6, gamma=2.2):\n","            self.power = power\n","            self.gamma = gamma\n","\n","        def __call__(self, image):\n","            image_dtype = image.dtype\n","\n","            if self.gamma is not None:\n","                image = image.astype('uint8')\n","                look_up_table = np.ones((256, 1), dtype='uint8') * 0\n","                for i in range(256):\n","                    look_up_table[i][0] = 255 * pow(i / 255, 1 / self.gamma)\n","                image = cv2.LUT(image, look_up_table)\n","\n","            image = image.astype('float32')\n","            image_power = np.power(image, self.power)\n","            rgb_vec = np.power(np.mean(image_power, (0, 1)), 1 / self.power)\n","            rgb_norm = np.sqrt(np.sum(np.power(rgb_vec, 2.0)))\n","            rgb_vec = rgb_vec / rgb_norm\n","            rgb_vec = 1 / (rgb_vec * np.sqrt(3))\n","            image = np.multiply(image, rgb_vec)\n","\n","            # Andrew Anikin suggestion\n","            image = np.clip(image, a_min=0, a_max=255)\n","\n","            return image.astype(image_dtype)\n","    \n","class CropBlackCircle(object):\n","    \"\"\"\n","    https://stackoverflow.com/questions/61986407/crop-x-ray-image-to-remove-black-background \n","    \"\"\"\n","    def __call__(self, image):\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","        # threshold \n","        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n","        hh, ww = thresh.shape\n","\n","        # make bottom 2 rows black where they are white the full width of the image\n","        thresh[hh-3:hh, 0:ww] = 0\n","\n","        # get bounds of white pixels\n","        white = np.where(thresh==255)\n","        xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n","\n","        # crop the image at the bounds adding back the two blackened rows at the bottom\n","        crop = image[ymin:ymax+3, xmin:xmax]\n","\n","        return crop\n"]},{"cell_type":"code","source":["# cropping out black border unfortunately Gessert uses Matlab for doing this https://github.com/ngessert/isic2019/blob/2394aaf60e7e070a5d0197a381426a1259a5f1bb/Matlab/adjust_2019.m \n","\n","# \"We binarize the images with a very low threshold, such that the entire dermoscopy field of view is set to 1. Then, we find the center of mass and the major and minor axis of an ellipse that has the same second central moments as the inner area. Based on these values we derive a rectangular bounding box for cropping that covers the\n","# relevant field of view. We automatically determin the necessecity for cropping based on a heuristic that tests whether the mean intensity inside the bounding box is substantially different from the mean intensity outside of the bounding box.\"\n","\n","# ALTERNATIVELY THIS METHOD ON stack OVERFLOW https://stackoverflow.com/questions/61986407/crop-x-ray-image-to-remove-black-background \n","\n","# could develop this by cropping the image more to omit more of the black egdes or look into inpainting https://github.com/fitushar/Skin-lesion-Segmentation-using-grabcut/blob/master/Skin%20lesion%20segmentation%20using%20grabcut%20in%20HSV%20color%20space.pdf \n","\n","\n","\n"],"metadata":{"id":"MdDUq0oS7gqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Augmentation \n","\n","# # CURRENT \n","# transform=transforms.Compose([\n","#                             transforms.RandomVerticalFlip(0.5),\n","#                             transforms.RandomHorizontalFlip(0.5),\n","#                             transforms.RandomApply(nn.ModuleList([transforms.ColorJitter(),\n","#                                                             transforms.GaussianBlur(3)]), p=0.1),\n","#                             ImageResize(224,224),\n","#                             transforms.PILToTensor(),\n","#                             transforms.ConvertImageDtype(torch.float),\n","#                             transforms.Normalize(*Imagenet_NV),\n","# )\n","\n","# # From Gessert however this doesnt match up with the paper??? The paper also mentions shear and other augmentation .... in the paper it says We use random brightness and contrast changes, random flipping, random rotation, random scaling (with appropriate\n","# # padding/cropping), and random shear. Furthermore, we use CutOut [7] with one hole and a hole size of 16. \n","\n","# # not sure about the random shear but could be RandomAffine, also not sure about random sclaing..could be RandomCrop- dont know if this has the padding/cropping ALSO I like to idea of wrapping some of these in RandomApply so that it only applies some \n","# #of these features with a given probability i.e. in 0.5 times of cases- dont want it to be on EVERY ONE- for example as used above, on ColourJitter and GuassianBlur and also techniqcally the flips have it too as they are set to 0.5-- this mean some of the data is \n","# # left alone too which is good \n","\n","# transform = transforms.Compose([\n","#                     cropping,\n","#                     transforms.RandomHorizontalFlip(),\n","#                     transforms.RandomVerticalFlip(),\n","#                     transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n","#                     transforms.RandomRotation(degrees=(0, 360)),\n","#                     transforms.ToTensor(),\n","#                     transforms.Normalize(torch.from_numpy(self.setMean).float(),torch.from_numpy(np.array([1.,1.,1.])).float())\n","#                     ])   \n","\n","# # based on above discssions I propose this...however not sure what probabilty of applying these? I cant tell from Gessarts paper ..he just said \"extensive\" \n","\n","# transform=transforms.Compose([\n","#                             transforms.RandomVerticalFlip(0.5),\n","#                             transforms.RandomHorizontalFlip(0.5),\n","#                             transforms.RandomApply(nn.ModuleList([ColorJitter(brightness=32. / 255.,saturation=0.5),\n","#                                                             transforms.RandomRotation(degrees=(0, 360)]), p=0.1),\n","#                             #CutOut(n_holes=1,length=16),\n","#                             ImageResize(224,224),\n","#                             transforms.PILToTensor(),\n","#                             transforms.ConvertImageDtype(torch.float),\n","#                             transforms.Normalize(*Imagenet_NV),\n","# )\n","\n","# # AutoAugment , code example of useage is here https://towardsdatascience.com/how-to-improve-your-image-classifier-with-googles-autoaugment-77643f0be0c9 \n","# # want to try using the Three autoaugment pretrined polices (for ImageNet, CIFAR10, SVHM) https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py \n","# # also examples of how to use Autoaugment with Pytorch https://github.com/DeepVoltaire/AutoAugment \n","\n","# # further exploration of a paper that just focuses on augmentation of skin lesions https://github.com/fabioperez/skin-data-augmentation/blob/91c6109796b23b0bf174632937474853b9559460/auglib/augmentation/augmentations.py#L71 "],"metadata":{"id":"XlRSakw0jop6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gessert also inclues CutOut with is added into the transforms as (Cutout_v0(n_holes=1,length=self.mdlParams['cutout']))\n","\n","class Cutout_v0(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        img = np.array(img)\n","        #print(img.shape)\n","        h = img.shape[0]\n","        w = img.shape[1]\n","\n","        mask = np.ones((h, w), np.uint8)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        #mask = torch.from_numpy(mask)\n","        #mask = mask.expand_as(img)\n","        img = img * np.expand_dims(mask,axis=2)\n","        img = Image.fromarray(img)\n","        return img    "],"metadata":{"id":"yZdc7uvnly9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TESTING HAIR REMOVAL FEATURE and black circle cut out\n","def hairremoval(image):\n","    # convert image to grayScale\n","    grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","\n","    # kernel for morphologyEx\n","    kernel = cv2.getStructuringElement(1, (17, 17))\n","\n","    # apply MORPH_BLACKHAT to grayScale image\n","    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","\n","    # apply thresholding to blackhat\n","    _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n","\n","    # inpaint with original image and threshold image\n","    final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n","\n","    return final_image\n","\n","def CropCircle(image):\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","        # threshold \n","        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n","        hh, ww = thresh.shape\n","\n","        # make bottom 2 rows black where they are white the full width of the image\n","        thresh[hh-3:hh, 0:ww] = 0\n","\n","        # get bounds of white pixels\n","        white = np.where(thresh==255)\n","        xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n","\n","        # crop the image at the bounds adding back the two blackened rows at the bottom\n","        crop = image[ymin:ymax+3, xmin:xmax]\n","\n","        return crop"],"metadata":{"id":"lY1xewrPkqRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# img = cv2.imread('/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_Split_val/test/AK/ISIC_0060404.jpg')\n","# cut_out_img = CropCircle(img)\n","# #cv2_imshow(img)\n","# test_hair_removed = hairremoval(cut_out_img)\n","# cv2_imshow(test_hair_removed)"],"metadata":{"id":"NaUZiQgLkuUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Testing out Shades of Grey\n","import os\n","import matplotlib.pyplot as plt\n","from skimage import io\n","import matplotlib.image as mpimg\n","import numpy as np\n","\n","\n","# img = cv2.imread('/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_Split_val/test/AK/ISIC_0060404.jpg')\n","# shadesofgrey = ShadesOfGrey()(img)\n","# cv2_imshow(shadesofgrey)\n","\n","# ## TRYING TO LOOP THROUGH IMAGES AND APPLY SHADES OF GREY TO THEN TRY OUT THE EFEFCTS OF CHANGING SHADES OF GREY PARAMETRS AND SEE IF THAT HELPS ....\n","# fig = plt.figure(figsize=(10,200))\n","# subplot_index = 1\n","# directory = \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro/\"\n","\n","# for filename in os.listdir(directory)[:10]:\n","#     original_image = io.imread(filename)\n","#     shadesofgrey = ShadesOfGrey()(original_image)\n","#     no_images = 11\n","#     ax = fig.add_subplot(no_images*3, 3, subplot_index)\n","#     ax.imshow(original_image)\n","#     #ax.set_title(f\"Poor image: {image_file[-11:-4]} score: {round(score)}\")\n","#     subplot_index += 1\n","\n","# plt.tight_layout()\n","# plt.show()\n","\n","\n","\n","# for root, dirs, files in os.walk(original_root_path, topdown=True):\n","#     print(f\"Processing {root}\")\n","#     for name in files:\n","#         original_image_path = os.path.join(root, name)\n"],"metadata":{"id":"fOA3n5DFbBrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## SORTING OUT EXTRA CODE FOR IMBALSAMPLER SO THAT ONLY THE TOP X % OF IMAGES ARE PASSED TO IT \n","\n","with open(\"/content/drive/MyDrive/Dissertation/skin_lesion_data/brisque_metrics.json\", \"r\") as f:\n","    brisque_metrics_reopened = json.load(f)"],"metadata":{"id":"cu193nDAkD91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.datasets import ImageFolder\n","test_train_set = ImageFolder(\"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro/train\")\n","train_image_paths = [os.path.basename(img_path) for img_path, label in test_train_set.imgs]\n","\n","train_brisque_scores = {}\n","for image_path, score in brisque_metrics_reopened.items():\n","    if os.path.basename(image_path) in train_image_paths:\n","        train_brisque_scores[os.path.basename(image_path)] = score"],"metadata":{"id":"P-TwfGvNBZA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Dissertation/skin_lesion_data/train_brisque_metrics.json\", \"w\") as f:\n","    json.dump(train_brisque_scores, f, indent=4)"],"metadata":{"id":"htoKlG7iPDE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Dissertation/skin_lesion_data/brisque_metrics.json\", \"r\") as f:\n","    train_brisque_metrics_reopened = json.load(f)"],"metadata":{"id":"1dhG-Ud0WJSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# structure is a dict, with file name then BRISQUE score\n","# need to pass a dataset to the ImbalancedSampler that has in X specified from BRISQUE score i.e. only the images in the top 90%\n","\n","from operator import itemgetter\n","brisque_dict = train_brisque_metrics_reopened\n","directory = \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro/\"\n","\n","def top_brisque (directory, brisque_dict, n):\n","    # create dictionary of top n image:value pairs \n","    n = int(len(brisque_dict) * 0.10)  # floor float result, as you must use an integer\n","    res = dict(sorted(brisque_dict.items(), key=itemgetter(1), reverse=True)[:n])\n","    \n"],"metadata":{"id":"TMeUqvTUWFiT"},"execution_count":null,"outputs":[]}]}