{"cells":[{"cell_type":"markdown","source":["# Multi-class classification\n","This file contains the custom classes, such as ImageResize() and the several Custom data samplers tested in this project.\n","\n","It then includes the pre-processing steps, such loading the data and augmentations.\n","\n","The network is built using densenet arhictecture, and then trained.\n","\n","The model includes validation and is finally tested. \n","\n","Weights and Biases is used to monitor all metrics."],"metadata":{"id":"tNGmbiG1hLED"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"opp4Cld9gck7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660734955898,"user_tz":-60,"elapsed":31892,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}},"outputId":"336a37e4-65cb-43ff-83fa-fda6b1ebb569"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.8 MB 16.1 MB/s \n","\u001b[K     |████████████████████████████████| 509 kB 63.6 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 54.5 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 58.4 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.1 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 50.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 57.1 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 54.3 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 52.9 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 44.7 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 793 kB 16.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 53.5 MB/s \n","\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Using device cuda:0\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fde0620c7f0>"]},"metadata":{},"execution_count":1}],"source":["!pip install -r /content/drive/MyDrive/Dissertation/requirements.txt -qqq\n","!pip install ipdb -qqq\n","\n","\n","from datetime import datetime\n","import json\n","import torch\n","import torchvision\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import os\n","import numpy as np\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import argmax\n","from tqdm import tqdm\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import seaborn as sn\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score, classification_report, balanced_accuracy_score\n",")\n","from torchsampler import ImbalancedDatasetSampler\n","import wandb\n","import numpy as np\n","from os import listdir\n","from os.path import join, isdir\n","from glob import glob\n","import cv2\n","import timeit\n","import timm\n","from sklearn.metrics import confusion_matrix\n","from torchvision.datasets import ImageFolder\n","from collections import Counter\n","from sklearn.utils import class_weight\n","# from autoaugment import ImageNetPolicy\n","# from autoaugment import CIFAR10Policy\n","# from autoaugment import SVHNPolicy\n","from torchvision.transforms import AutoAugment\n","from operator import itemgetter\n","import random\n","\n","from torchvision.transforms.autoaugment import AutoAugmentPolicy\n","\n","#changing the device to GPU rather than CPU if it is available,\n","# this will decrease model training time.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device {}\".format(device))\n","\n","torch.manual_seed(17)"]},{"cell_type":"markdown","metadata":{"id":"-qLIgh8FkgiD"},"source":["Several custom classes were made. The first, ImageResize(), resizes an image using the PIL image resizer set to the bilinear function. A custom class was generated even though Pytorch has its own resizer, as discussed in the following article, https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing there are resizing issues associated with certain libraries such as Pytorch which introduce artefacts or don't provide sufficient antialiasing in the resized images using libraries such as Pytorch.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b9TywHjhgu2P","executionInfo":{"status":"ok","timestamp":1660734955899,"user_tz":-60,"elapsed":10,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["class CustomImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, top_percent_keep, path_brisque_score):\n","        from pdb import set_trace; set_trace()\n","        # mess with dataset\n","        with open(path_brisque_score, \"r\") as f:\n","            image_brisque_metrics = json.load(f)\n","        dataset_names = dataset.imgs\n","        n = int(len(image_brisque_metrics) * top_percent_keep)\n","        top_percent_brisque = dict(sorted(image_brisque_metrics.items(), key=itemgetter(1), reverse=True)[:n])\n","        top_percent_names = list(top_percent_brisque.keys())\n","        top_percent_names = [os.path.basename(x) for x in top_percent_names]\n","        \n","        filtered_dataset_names = []\n","        for img_path, label in dataset_names:\n","            if os.path.basename(img_path) in top_percent_names:\n","               filtered_dataset_names.append((img_path, label))\n","        \n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        from pdb import set_trace; set_trace()\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)\n","\n","\n","class ClassBasedCustomImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, top_percent_keep, path_brisque_score):\n","        # mess with dataset\n","        with open(path_brisque_score, \"r\") as f:\n","            image_brisque_metrics = json.load(f)\n","        dataset_names = dataset.imgs\n","        \n","        labels = []\n","        for key, _ in image_brisque_metrics.items():\n","            label = key.split(\"/\")[-2]\n","            labels.append(label)\n","\n","        labels = set(labels)\n","\n","        class_dictionaries = []\n","\n","        for label in labels:\n","            class_images = {}\n","            for key, value in image_brisque_metrics.items():\n","                image_label = key.split(\"/\")[-2]\n","                if image_label == label:\n","                    class_images[key] = value\n","            class_dictionaries.append(class_images)\n","    \n","        final_top_percent_names = []\n","\n","        for dictionary in class_dictionaries:\n","            n = int(len(dictionary) * top_percent_keep)\n","            top_percent_brisque = dict(sorted(dictionary.items(), key=itemgetter(1), reverse=True)[:n])\n","            top_percent_names = list(top_percent_brisque.keys())\n","            top_percent_names = [os.path.basename(x) for x in top_percent_names]\n","            final_top_percent_names.extend(top_percent_names)\n","  \n","        filtered_dataset_names = []\n","        for img_path, label in dataset_names:\n","            if os.path.basename(img_path) in final_top_percent_names:\n","               filtered_dataset_names.append((img_path, label))\n","        \n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)\n","\n","class ClassBasedRandomImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, random_percent_keep, path_brisque_score):\n","        # mess with dataset\n","        with open(path_brisque_score, \"r\") as f:\n","            image_brisque_metrics = json.load(f)\n","        dataset_names = dataset.imgs\n","        \n","        labels = []\n","        for key, _ in image_brisque_metrics.items():\n","            label = key.split(\"/\")[-2]\n","            labels.append(label)\n","\n","        labels = set(labels)\n","\n","        class_dictionaries = []\n","\n","        for label in labels:\n","            class_images = {}\n","            for key, value in image_brisque_metrics.items():\n","                image_label = key.split(\"/\")[-2]\n","                if image_label == label:\n","                    class_images[key] = value\n","            class_dictionaries.append(class_images)\n","    \n","        final_random_percent_names = []\n","\n","        for dictionary in class_dictionaries:\n","            n = int(len(dictionary) * random_percent_keep)\n","            random_percent = dict(random.sample(dataset_names, k = len(dataset_names))[:n])\n","            random_percent_names = list(random_percent.keys())\n","            random_percent_names = [os.path.basename(x) for x in random_percent_names]\n","            final_random_percent_names.extend(random_percent_names)\n","  \n","        filtered_dataset_names = []\n","        for img_path, label in dataset_names:\n","            if os.path.basename(img_path) in final_random_percent_names:\n","               filtered_dataset_names.append((img_path, label))\n","        \n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)\n","\n","\n","\n","class WorstCustomImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, top_percent_keep, path_brisque_score):\n","        # mess with dataset\n","        with open(path_brisque_score, \"r\") as f:\n","            image_brisque_metrics = json.load(f)\n","        dataset_names = dataset.imgs\n","        n = int(len(image_brisque_metrics) * top_percent_keep)\n","        top_percent_brisque = dict(sorted(image_brisque_metrics.items(), key=itemgetter(1), reverse=False)[:n])\n","        top_percent_names = list(top_percent_brisque.keys())\n","        \n","        filtered_dataset_names = []\n","        for img_path, label in dataset_names:\n","            if os.path.basename(img_path) in top_percent_names:\n","               filtered_dataset_names.append((img_path, label))\n","        \n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)\n","\n","class RandomImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, random_percent_keep):\n","        # mess with dataset\n","        dataset_names = dataset.imgs\n","        n = int(len(dataset_names) * random_percent_keep)\n","        random_percent = dict(random.sample(dataset_names, k = len(dataset_names))[:n])\n","        random_percent_names = list(random_percent .keys())\n","        random_percent_names = [os.path.basename(x) for x in random_percent_names]\n","        \n","        filtered_dataset_names = []\n","        for img_path, label in dataset_names:\n","            if os.path.basename(img_path) in random_percent_names:\n","               filtered_dataset_names.append((img_path, label))\n","        \n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)\n","\n","class ClassBasedWorstImbalancedDatasetSampler(ImbalancedDatasetSampler):\n","    def __init__(self, dataset, callback_get_label, percent_keep, path_brisque_score):\n","        # mess with dataset\n","        with open(path_brisque_score, \"r\") as f:\n","            image_brisque_metrics = json.load(f)\n","        dataset_names = dataset.imgs\n","        \n","        labels = []\n","        for key, _ in image_brisque_metrics.items():\n","            label = key.split(\"/\")[-2]\n","            labels.append(label)\n","\n","        labels = set(labels)\n","\n","        class_dictionaries = []\n","\n","        for label in labels:\n","            class_images = {}\n","            for key, value in image_brisque_metrics.items():\n","                image_label = key.split(\"/\")[-2]\n","                if image_label == label:\n","                    class_images[key] = value\n","            class_dictionaries.append(class_images)\n","    \n","        final_worst_percent_names = []\n","\n","        for dictionary in class_dictionaries:\n","            n = int(len(dictionary) * percent_keep)\n","            worst_percent_brisque = dict(sorted(dictionary.items(), key=itemgetter(1), reverse=False)[:n])\n","            worst_percent_names = list(worst_percent_brisque.keys())\n","            worst_percent_names = [os.path.basename(x) for x in worst_percent_names]\n","            final_worst_percent_names.extend(worst_percent_names)\n","  \n","        filtered_dataset_names = []\n","        for img_path, label in dataset_names:\n","            if os.path.basename(img_path) in final_worst_percent_names:\n","               filtered_dataset_names.append((img_path, label))\n","        \n","        dataset.imgs = filtered_dataset_names \n","        # pass back up\n","        dataset.targets = [x[1] for x in dataset.imgs]\n","        dataset.samples = dataset.imgs\n","        super().__init__(dataset=dataset, callback_get_label=callback_get_label)\n","\n","class PILtoCV2(object):\n","\n","    def __call__(self, image):\n","        return cv2.cvtColor(\n","            np.array(image),\n","            cv2.COLOR_RGB2BGR\n","        )\n","\n","\n","class CV2toPIL(object):\n","\n","    def __call__(self, image):\n","        color_coverted = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return Image.fromarray(color_coverted)\n","\n","\n","class ImageResize(object):\n","        \"\"\"\n","        PIL's resize performs better than pytorch\n","        https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing\n","        \"\"\"\n","\n","        def __init__(self, new_h, new_w):\n","            self.new_h = new_h\n","            self.new_w = new_w\n","\n","        def __call__(self, image):\n","            image = image.resize((self.new_w, self.new_h), resample=Image.BILINEAR)\n","            return image\n","\n","class HairRemoval(object):\n","        \"\"\"\n","        Hair removal code\n","        https://github.com/ThiruRJST/Melanoma_Classification \n","        \"\"\"\n","\n","        def __call__(self, image):\n","            start_time = datetime.now()\n","            # convert image to grayScale\n","            grayScale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","            # kernel for morphologyEx\n","            kernel = cv2.getStructuringElement(1, (17, 17))\n","\n","            # apply MORPH_BLACKHAT to grayScale image\n","            blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","\n","            # apply thresholding to blackhat\n","            _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n","\n","            # inpaint with original image and threshold image\n","            final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n","            # print(\"Hair removal\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","            return final_image\n","\n","\n","class ShadesOfGrey(object):\n","        \"\"\"\n","        Code from https://github.com/ThiruRJST/Melanoma_Classification\n","        imgage (numpy array): the original image with format of (h, w, c)\n","        power (int): the degree of norm, 6 is used in reference paper\n","        gamma (float): the value of gamma correction, 2.2 is used in reference paper\n","        \"\"\"\n","\n","        def __init__(self, power=6, gamma=2.2):\n","            self.power = power\n","            self.gamma = gamma\n","\n","        def __call__(self, image):\n","            start_time = datetime.now()\n","            image_dtype = image.dtype\n","\n","            if self.gamma is not None:\n","                image = image.astype('uint8')\n","                look_up_table = np.ones((256, 1), dtype='uint8') * 0\n","                for i in range(256):\n","                    look_up_table[i][0] = 255 * pow(i / 255, 1 / self.gamma)\n","                image = cv2.LUT(image, look_up_table)\n","\n","            image = image.astype('float32')\n","            image_power = np.power(image, self.power)\n","            rgb_vec = np.power(np.mean(image_power, (0, 1)), 1 / self.power)\n","            rgb_norm = np.sqrt(np.sum(np.power(rgb_vec, 2.0)))\n","            rgb_vec = rgb_vec / rgb_norm\n","            rgb_vec = 1 / (rgb_vec * np.sqrt(3))\n","            image = np.multiply(image, rgb_vec)\n","\n","            # Andrew Anikin suggestion\n","            image = np.clip(image, a_min=0, a_max=255)\n","            # print(\"Shades of grey\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","            return image.astype(image_dtype)\n","    \n","class CropBlackCircle(object):\n","    \"\"\"\n","    https://stackoverflow.com/questions/61986407/crop-x-ray-image-to-remove-black-background \n","    \"\"\"\n","    def __call__(self, image):\n","        start_time = datetime.now()\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","        # threshold \n","        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n","        hh, ww = thresh.shape\n","\n","        # make bottom 2 rows black where they are white the full width of the image\n","        thresh[hh-3:hh, 0:ww] = 0\n","\n","        # get bounds of white pixels\n","        white = np.where(thresh==255)\n","        xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n","\n","        # crop the image at the bounds adding back the two blackened rows at the bottom\n","        crop = image[ymin:ymax+3, xmin:xmax]\n","        # print(\"Crop black circle\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","        return crop\n","\n","class CutOut(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","        Args:\n","            n_holes (int): Number of patches to cut out of each image.\n","            length (int): The length (in pixels) of each square patch.\n","        \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        #start_time = datetime.now()\n","        img = np.array(img)\n","        # print(img.shape)\n","        h = img.shape[0]\n","        w = img.shape[1]\n","\n","        mask = np.ones((h, w), np.uint8)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        #mask = torch.from_numpy(mask)\n","        #mask = mask.expand_as(img)\n","        img = img * np.expand_dims(mask,axis=2)\n","        #print(\"Cut out\", (datetime.now() - start_time).total_seconds(), \"seconds\")\n","        return img  "]},{"cell_type":"markdown","metadata":{"id":"ZeVHonwolOyZ"},"source":["## Network generation\n","\n","Five networks were created using several architectures loaded from the Pytroch library.  Within each model-specific function, the feature extraction layers have been frozen and the final classification layer unfrozen, to allow this final layer to train on our Lesion dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bxMWZ9CIlK7B","executionInfo":{"status":"ok","timestamp":1660734955900,"user_tz":-60,"elapsed":10,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["def create_network_densenet():\n","        net = models.densenet161(pretrained=True)\n","        net.classifier = nn.Linear(in_features=2208, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n","\n","\n","def create_network_efficientnet():\n","        net = models.efficientnet_b0(pretrained=True)\n","        net.classifier[1] = nn.Linear(in_features=1280, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","                param.requires_grad = False\n","        for param in net.classifier.parameters():\n","                param.requires_grad = True\n","\n","        return net\n","\n","def create_network_mobilenet():\n","        net = models.mobilenet_v3_small(pretrained=True)\n","        net.classifier[3] = nn.Linear(in_features=1024, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","            param.requires_grad = False\n","        for param in net.classifier.parameters():\n","            param.requires_grad = True\n","\n","        return net\n","\n","\n","def create_network_resnet():\n","        net = models.resnet18(pretrained=True)\n","        net.fc = nn.Linear(in_features=512, out_features=8, bias=True)\n","\n","        for param in net.parameters():\n","            param.requires_grad = False\n","        for param in net.fc.parameters():\n","            param.requires_grad = True\n","\n","        return net\n"]},{"cell_type":"code","source":["class ImageFolderWithPaths(ImageFolder):\n","\n","    def __getitem__(self, index: int):\n","        path, target = self.samples[index]\n","        sample = self.loader(path)\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return sample, target, path"],"metadata":{"id":"FYQuy_tlq9BV","executionInfo":{"status":"ok","timestamp":1660734955902,"user_tz":-60,"elapsed":11,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xDTi6y0Uk7N9","executionInfo":{"status":"ok","timestamp":1660734956353,"user_tz":-60,"elapsed":8,"user":{"displayName":"Sarah-Louise Hayes","userId":"13751648430878105914"}}},"outputs":[],"source":["def run(config):  \n","    torch.manual_seed(17)\n","    BASE_PATH = '/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_1_models/'\n","    now = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\")\n","    RESULTS_PATH = os.path.join(BASE_PATH, now)\n","    os.makedirs(RESULTS_PATH, exist_ok=False)\n","\n","    with open(os.path.join(RESULTS_PATH, \"config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=4)\n","\n","    if config.get(\"use_wandb\"):\n","        # this integrates the third-party platform, Weights and Biases,\n","        # with this notebook.\n","        run = wandb.init(\n","            project=\"part_1_skin_lesion\",\n","            entity=\"sarahlouise\",\n","            config=config,\n","        )\n","\n","\n","    # ## Train, validation and test set\n","    #\n","    # The train, validation and test sets were loaded and transformed using the Pytorch functions.\n","\n","    batch_size = config[\"batch_size\"]\n","    Imagenet_NV = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","    print(\"Configuring datasets\")\n","    trainset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'train'),\n","        transform=transforms.Compose([\n","                                   ## v2 own inspired   \n","                            #           transforms.RandomVerticalFlip(0.5),\n","                            # transforms.RandomHorizontalFlip(0.5),\n","                            # transforms.RandomApply(nn.ModuleList([transforms.ColorJitter(),\n","                            #                                 transforms.GaussianBlur(3)]), p=0.1),\n","                            ## v1 Gessert Inspired \n","                            # ImageResize(224,224),\n","                            # transforms.RandomVerticalFlip(0.5),\n","                            # transforms.RandomHorizontalFlip(0.5),\n","                            # transforms.RandomApply(\n","                            #     nn.ModuleList([\n","                            #                    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n","                            #                    transforms.RandomRotation(degrees=(0, 360)),\n","                                               #transforms.CenterCrop(100)\n","                                            #    ]), p=0.1),\n","                            # transforms.AutoAugment(AutoAugmentPolicy.IMAGENET),\n","                            ## v3 own inspired\n","                            transforms.RandomVerticalFlip(0.5),\n","                            transforms.RandomHorizontalFlip(0.5),\n","                            transforms.RandomApply(nn.ModuleList([transforms.RandomRotation(degrees=(0, 360))]), p=0.2),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","\n","                        ])\n","        )\n","    \n","    imbalanced_sampler_choice = [\n","        config.get(\"use_custom_imbalanced_sampler\"),\n","        config.get(\"use_imbalanced_sampler\"),\n","        config.get(\"use_RandomImbalancedDatasetSampler\"),\n","        config.get(\"WorstCustomImbalancedDatasetSampler\"),\n","        config.get(\"ClassBasedCustomImbalancedDatasetSampler\"),\n","        config.get(\"ClassBasedRandomImbalancedDatasetSampler\"),\n","        config.get(\"ClassBasedWorstImbalancedDatasetSampler\"),\n","    ]\n","    if imbalanced_sampler_choice.count(True) > 1:\n","        raise ValueError(\"Pick one sampler, cant use all\")\n","\n","\n","    # if config.get(\"use_custom_imbalanced_sampler\") and config.get(\"use_imbalanced_sampler\") and config.get(\"use_RandomImbalancedDatasetSampler\") and config.get(\"WorstCustomImbalancedDatasetSampler\"):\n","    #     raise ValueError(\"Pick one sampler, cant use all\")\n","\n","    elif config.get(\"use_custom_imbalanced_sampler\"):\n","        sampler = CustomImbalancedDatasetSampler(\n","            dataset=trainset,\n","            callback_get_label=lambda x: x.targets,\n","            top_percent_keep=config.get(\"percent_keep\"), \n","            path_brisque_score=\"/content/drive/MyDrive/Dissertation/skin_lesion_data/v2_prepro_brisque_metrics.json\",\n","        )\n","        shuffle = False\n","    elif config.get(\"use_imbalanced_sampler\"):\n","        sampler = ImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets,\n","        )\n","        shuffle = False\n","    elif config.get(\"use_RandomImbalancedDatasetSampler\"):\n","        sampler = RandomImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets,\n","            random_percent_keep=config.get(\"percent_keep\"),\n","        )\n","        shuffle = False\n","    elif config.get(\"WorstCustomImbalancedDatasetSampler\"):\n","        sampler = RandomImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets,\n","            random_percent_keep=config.get(\"percent_keep\"),\n","        )\n","        shuffle = False\n","    elif config.get(\"ClassBasedCustomImbalancedDatasetSampler\"):\n","        sampler = ClassBasedCustomImbalancedDatasetSampler(\n","            dataset=trainset,\n","            callback_get_label=lambda x: x.targets,\n","            top_percent_keep=config.get(\"percent_keep\"), \n","            path_brisque_score=\"/content/drive/MyDrive/Dissertation/skin_lesion_data/v2_prepro_brisque_metrics.json\",\n","        )\n","        shuffle = False\n","    elif config.get(\"ClassBasedRandomImbalancedDatasetSampler\"):\n","        sampler = ClassBasedRandomImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets,\n","            random_percent_keep=config.get(\"percent_keep\"),\n","            # Only using to get class breakdown\n","            path_brisque_score=\"/content/drive/MyDrive/Dissertation/skin_lesion_data/v2_prepro_brisque_metrics.json\",\n","        )\n","        shuffle=False\n","    elif config.get(\"ClassBasedWorstImbalancedDatasetSampler\"):\n","        sampler = ClassBasedWorstImbalancedDatasetSampler(\n","            trainset,\n","            callback_get_label=lambda x: x.targets,\n","            percent_keep=config.get(\"percent_keep\"),\n","            path_brisque_score=\"/content/drive/MyDrive/Dissertation/skin_lesion_data/v2_prepro_brisque_metrics.json\",\n","        )\n","        shuffle = False\n","    else:\n","        sampler = None\n","        shuffle = True\n","\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        trainset,\n","        batch_size=batch_size,\n","        num_workers=2,\n","        shuffle=shuffle,\n","        sampler=sampler,\n","    )\n","\n","    valset = ImageFolder(\n","        os.path.join(config.get(\"data_path\"), 'val'),\n","        transform=transforms.Compose([\n","                            # PILtoCV2(),\n","                            # HairRemoval(),\n","                            # CropBlackCircle(),\n","                            # ShadesOfGrey(),\n","                            # CV2toPIL(),\n","                            ImageResize(224,224),\n","                            transforms.PILToTensor(),\n","                            transforms.ConvertImageDtype(torch.float),\n","                            transforms.Normalize(*Imagenet_NV),\n","                        ]))\n","\n","    valloader = torch.utils.data.DataLoader(\n","        valset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","\n","    testset = ImageFolderWithPaths(\n","        os.path.join(config.get(\"data_path\"), 'test'),\n","        transform=transforms.Compose([\n","                                # PILtoCV2(),\n","                                # HairRemoval(),\n","                                # CropBlackCircle(),\n","                                # ShadesOfGrey(),\n","                                # CV2toPIL(),\n","                                ImageResize(224,224),\n","                                transforms.PILToTensor(),\n","                                transforms.ConvertImageDtype(torch.float),\n","                                transforms.Normalize(*Imagenet_NV),\n","                            ]))\n","\n","    testloader = torch.utils.data.DataLoader(\n","        testset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","    \n","    classes = sorted(os.listdir(os.path.join(config.get(\"data_path\"), 'train')))\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.sklearn.plot_class_proportions(\n","            trainset.targets,\n","            testset.targets,\n","            classes\n","        )\n","\n","\n","    #making logical arguments from the configuration dictionary definied earlier\n","    model_name = config[\"model\"]\n","\n","    if model_name == \"densenet\":\n","        net = create_network_densenet()\n","    elif model_name == \"efficientnet\":\n","        net = create_network_efficientnet()\n","    elif model_name == \"mobilenet\":\n","        net = create_network_mobilenet()\n","    elif model_name == \"resnet\":\n","        net = create_network_resnet()\n","    else:\n","        raise ValueError(\"Model name not supported '{}'\".format(model_name))\n","\n","    print(f\"Moving network to {device}\")\n","    net = net.to(device)\n","\n","    # ## Training configurations\n","    \n","    # defining the training configurations\n","    no_of_epochs = config[\"epochs\"]\n","    \n","    # calculating class weights\n","\n","    labels = np.array(trainset.targets)\n","    \n","    class_weights = class_weight.compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(labels),\n","        y=labels\n","    )\n","    class_weights = torch.tensor(\n","        class_weights,\n","        dtype=torch.float\n","    )\n","\n","    if config.get(\"use_weighted_loss\"):\n","        criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n","    else:\n","        criterion = nn.CrossEntropyLoss().to(device)\n","\n","    optimizer = optim.Adam(\n","        net.parameters(),\n","        lr=config[\"lr\"],\n","        amsgrad=config[\"use_amsgrad\"]\n","    )\n","\n","    if config[\"use_warm_restarts\"]:\n","        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","            optimizer,\n","            T_0=10,\n","            T_mult=1,\n","        )\n","    else:\n","        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","            optimizer,\n","            round((len(trainset)/batch_size)*no_of_epochs)\n","        )\n","\n","\n","    # ##  Model training\n","    #\n","    # This section included the training for the model. The network was set to train, a mini-batch was passed through the model and then the loss was then backpropagated. Each trained model was also automatically saved.\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.watch(net)\n","\n","    if not config.get(\"use_pretrained\"):\n","        print(\"Starting training\")\n","        for epoch in range(no_of_epochs):  # loop over the dataset multiple times\n","            print(\"Epoch {} of {}\".format(epoch+1, no_of_epochs))\n","            epoch_running_loss = []\n","            epoch_val_metric = []\n","            for i, data in tqdm(enumerate(trainloader, 1), total=len(trainloader)):\n","                current_step = (len(trainloader) * epoch) + i\n","                \n","                # get the inputs; data is a list of [inputs, labels]\n","                net.train()\n","                \n","                inputs, labels = data\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                \n","                # forward + backward\n","                outputs = net(inputs)\n","\n","                \n","                # outputs = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","                \n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                # saving loss statistics\n","                loss_val = loss.item()\n","                if config.get(\"use_wandb\"):\n","                    wandb.log(\n","                        {\n","                            \"train/bce_logits_loss\": loss_val,\n","                            \"train/lr\": scheduler.get_last_lr()[0],\n","                            \"train/custom_step\": (len(trainloader) * epoch) + i,\n","                        }\n","                    )\n","\n","                # adding in valiation data testing\n","                if i % round(40*config.get(\"percent_keep\", 1)) == 0:\n","                    model_name = \"{name}-epoch-{epoch}-step-{step}.pth\".format(\n","                        name=config[\"name\"],\n","                        epoch=epoch,\n","                        step=i,\n","                    )\n","                    model_path = os.path.join(RESULTS_PATH, model_name)\n","                    # Commenting out to save space\n","                    # torch.save(net.state_dict(), model_path)\n","\n","                    predicted = []\n","                    actual = []\n","\n","                    print(\"Running validation\")\n","                    net.eval()\n","                    with torch.no_grad():\n","                        for i, data in enumerate(tqdm(valloader, total=len(valloader))):\n","                            inputs, labels = data\n","                            inputs = inputs.to(device)\n","                            labels = labels.to(device)\n","\n","                            outputs = net(inputs)\n","                            loss = criterion(outputs, labels)\n","                            if config.get(\"use_wandb\"):\n","                                wandb.log({\n","                                    \"val/loss\": loss,\n","                                    \"val/custom_step\": current_step,\n","                                })\n","\n","                            # collect the correct predictions for each class\n","                            actual.extend(labels.detach().cpu().numpy())\n","                            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","\n","                        mb_acc = accuracy_score(actual, predicted)\n","                        precision = precision_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        recall = recall_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","                        bal_acc = balanced_accuracy_score(actual, predicted)\n","                        metric_report = classification_report(\n","                            np.array(actual),\n","                            np.array(predicted),\n","                            output_dict=True,\n","                            zero_division=1,\n","                        )\n","                        metric_report = {\n","                            f\"val_classes/{k}\": v\n","                            for k, v in metric_report.items()\n","                        }\n","                        metric_report.update({\n","                            \"val_classes/custom_step\": current_step,\n","                        })\n","\n","                        if config.get(\"use_wandb\"):\n","                            wandb.log({\n","                                \"val/accuracy\": mb_acc,\n","                                \"val/precision\": precision,\n","                                \"val/recall\": recall,\n","                                \"val/f1\": f1,\n","                                \"val/balanced_acc\": bal_acc,\n","                                \"val/custom_step\": current_step\n","                            })\n","                            wandb.log(metric_report)\n","\n","\n","\n","        print('Finished Training')\n","        # saving each trained model\n","        model_name = '{}-final.pth'.format(\n","            config[\"name\"]\n","        )\n","        model_path = os.path.join(RESULTS_PATH, model_name)\n","        torch.save(net.state_dict(), model_path)\n","\n","        print(\"Final model saved\")\n","\n","    # ##  Model testing\n","    #\n","    # The model was set to evaluate mode for testing. The predicted and actual results were saved, and used for generating a confusion matrix and metrics.\n","    #\n","    # A confusion matrix was generated, showing the number of true positives and negatives, and false positives and negatives. Additionally, the metrics were calculated, including the accuracy score, precision and recall. The corresponding graphs were created using Weights and Biases.\n","\n","\n","    if config.get(\"use_pretrained\"):\n","        net.load_state_dict(\n","            torch.load(\n","                config.get(\"use_pretrained\")\n","            )\n","        )\n","\n","    predicted = []\n","    actual = []\n","    image_paths = []\n","    \n","    print(\"Testing model\")\n","    # again no gradients needed\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(tqdm(testloader, total=len(testloader))):\n","            inputs, labels, paths = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = net(inputs)\n","            actual.extend(labels.detach().cpu().numpy())\n","            predicted.extend(torch.argmax(outputs, 1).detach().cpu().numpy())\n","            image_paths.extend(paths)\n","\n","\n","    results = pd.DataFrame.from_dict(\n","        {\n","            \"predicted\": predicted,\n","            \"actual\": actual,\n","            \"image_path\": image_paths\n","        }\n","    )\n","    results.to_csv(os.path.join(RESULTS_PATH, \"each_image_predictions.csv\"))\n","\n","\n","    print(\"Calculating performance metrics\")\n","    model_accuracy = accuracy_score(actual, predicted)\n","    model_precision = precision = precision_score(actual, predicted,\n","                                                average=\"macro\", zero_division = 1)\n","    model_recall = recall_score(actual, predicted,\n","                                average=\"macro\", zero_division = 1)\n","    model_f1 = f1_score(actual, predicted, average=\"macro\", zero_division = 1)\n","    model_bal_acc = balanced_accuracy_score(actual, predicted)\n","    metric_report = classification_report(\n","        np.array(actual),\n","        np.array(predicted),\n","        output_dict=True,\n","        zero_division=1,\n","    )\n","    metric_report = {\n","        f\"test_classes/{k}\": v\n","        for k, v in metric_report.items()\n","    }\n","\n","    if config.get(\"use_wandb\"):\n","        wandb.log({\"test/accuracy\": model_accuracy})\n","        wandb.log({\"test/precision\": model_precision})\n","        wandb.log({\"test/recall\": model_recall})\n","        wandb.log({\"test/f1\": model_f1})\n","        wandb.log({\"test/bal_acc\": model_bal_acc})\n","        wandb.sklearn.plot_confusion_matrix(actual, predicted, classes, normalize=\"true\")\n","        wandb.log(metric_report)\n","    else:\n","        print(\"test/accuracy\", model_accuracy)\n","        print(\"test/precision\", model_precision)\n","        print(\"test/recall\", model_recall)\n","        print(\"test/f1\", model_f1)\n","        print(\"test/bal_acc\", model_bal_acc)\n","\n","    plot_save_path = os.path.join(RESULTS_PATH, '{}_confusion_matrix.png'.format(\n","        config[\"name\"]\n","    ))\n","    print(\"Plotting confusion matrix in {}\".format(plot_save_path))\n","\n","    print(\"Calculating\")\n","    conf_matrix = confusion_matrix(y_true=actual, y_pred=predicted, normalize=\"true\")\n","    print(\"Done\")\n","    fig =plt.figure(figsize=(20, 20))\n","    ax = fig.add_subplot(1,1,1)\n","    ax.matshow(conf_matrix, cmap=plt.cm.Reds)\n","\n","\n","    plt.xlabel('Predictions')\n","    plt.ylabel('Actuals')\n","    plt.title('Confusion Matrix')\n","    plt.savefig(plot_save_path)\n","    # plt.show()\n","\n","    if config.get(\"use_wandb\"):\n","        run.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjSeEY5BUdey","colab":{"base_uri":"https://localhost:8080/","height":329,"referenced_widgets":["c01ea5d7c2e1466aa3161d06efde28b1","bc6a689a4fae4128ab420367a72468b1","7b33674dbef74554b500e00deb4112a3","c76dbe2fab334efbbd4832a7776ea654","e228cf06d9324a2dba82b224324d3b61","2be0861d40f943a4af159d09af1ef8cc","c411df6cfa284eb1bb1cd08631fec42e","3be25d5a3f0948e791bfda6e38587f7c","82ef5342650b46a988510bf0725f64b8","fdc77b0d39ee4afaadf206b0389029bd","cf0e726b67a44b2e8725ae1cf1716494"]},"outputId":"935be760-9a5e-436c-fd08-ae60d481b3eb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220817_111602-1sawtxr2</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion/runs/1sawtxr2\" target=\"_blank\">robust-sea-183</a></strong> to <a href=\"https://wandb.ai/sarahlouise/part_1_skin_lesion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Configuring datasets\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/110M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c01ea5d7c2e1466aa3161d06efde28b1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Moving network to cuda:0\n","Starting training\n","Epoch 1 of 5\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/80 [00:33<?, ?it/s]"]}],"source":["#from part_2 import run\n","#percentage_intervals = np.arange(0.1, 1, 0.1).tolist()\n","\n","#for x in percentage_intervals:\n","conf = {\n","    \"epochs\": 5,\n","    \"batch_size\": 256,\n","    \"lr\": 1e-2,\n","    \"name\": \"densenet\",\n","    \"model\": \"densenet\",\n","    \"use_wandb\": True,\n","    \"use_amsgrad\": True,\n","    \"use_warm_restarts\": False,\n","    \"use_weighted_loss\": False,\n","    \"data_path\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro\",\n","    \"use_imbalanced_sampler\": True,\n","    \"use_custom_imbalanced_sampler\": False,\n","    \"use_RandomImbalancedDatasetSampler\":False,\n","    \"WorstCustomImbalancedDatasetSampler\": False,\n","    \"ClassBasedCustomImbalancedDatasetSampler\": False,\n","    \"ClassBasedRandomImbalancedDatasetSampler\": False,\n","    \"ClassBasedWorstImbalancedDatasetSampler\": False,\n","    \"percent_keep\": round(1, 1),\n","    # \"use_pretrained\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_1_models/2022-05-27_23-23-02.393905/densenet-final.pth\",\n","}\n","run(conf)\n","\n","\n","# all_configs = [\n","#             {\n","#         \"epochs\": 5,\n","#         \"batch_size\": 256,\n","#         \"lr\": 1e-2,\n","#         \"name\": \"densenet\",\n","#         \"model\": \"densenet\",\n","#         \"use_wandb\": True,\n","#         \"use_amsgrad\": True,\n","#         \"use_warm_restarts\": False,\n","#         \"use_weighted_loss\": False,\n","#         \"data_path\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/ISIC_2019_v2_prepro\",\n","#         \"use_imbalanced_sampler\": False,\n","#         \"use_custom_imbalanced_sampler\": True,\n","#         \"top_percent_keep\": 0.1,\n","#         #\"use_pretrained\": \"/content/drive/MyDrive/Dissertation/skin_lesion_data/skin_lesion_part_1_models/2022-05-17_21-00-07.965720/densenet-final.pth\",\n","#     },\n","    \n","# ]\n","\n","# # for config in all_configs:\n","# #     run(config)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"quality_models.ipynb","provenance":[],"background_execution":"on","mount_file_id":"1MGy-NJAA8g75KYbGheTqnoQzj9Rl4iB4","authorship_tag":"ABX9TyN+XGCMYBH9fSFTIChzIBTe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c01ea5d7c2e1466aa3161d06efde28b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc6a689a4fae4128ab420367a72468b1","IPY_MODEL_7b33674dbef74554b500e00deb4112a3","IPY_MODEL_c76dbe2fab334efbbd4832a7776ea654"],"layout":"IPY_MODEL_e228cf06d9324a2dba82b224324d3b61"}},"bc6a689a4fae4128ab420367a72468b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2be0861d40f943a4af159d09af1ef8cc","placeholder":"​","style":"IPY_MODEL_c411df6cfa284eb1bb1cd08631fec42e","value":"100%"}},"7b33674dbef74554b500e00deb4112a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3be25d5a3f0948e791bfda6e38587f7c","max":115730790,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82ef5342650b46a988510bf0725f64b8","value":115730790}},"c76dbe2fab334efbbd4832a7776ea654":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdc77b0d39ee4afaadf206b0389029bd","placeholder":"​","style":"IPY_MODEL_cf0e726b67a44b2e8725ae1cf1716494","value":" 110M/110M [00:00&lt;00:00, 250MB/s]"}},"e228cf06d9324a2dba82b224324d3b61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2be0861d40f943a4af159d09af1ef8cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c411df6cfa284eb1bb1cd08631fec42e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3be25d5a3f0948e791bfda6e38587f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82ef5342650b46a988510bf0725f64b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdc77b0d39ee4afaadf206b0389029bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf0e726b67a44b2e8725ae1cf1716494":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}